{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba21609f-a060-4ddf-a8a1-af647a72a530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OASIS loading .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 415/415 [00:00<00:00, 254181.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host name rtx-06.nmr.mgh.harvard.edu\n",
      "using 0 gpus\n",
      "model_device /gpu:0, synth_device /gpu:0, dev_str 0\n",
      "physical GPU # is None\n",
      "dofit True, doaff True, fit_lin False, oshapes True, subnet loss False, use_lab2ind False, combined_training False, insertion True, concat aseg True, subloss dice\n",
      "TRAINING model with loss dice\n",
      "patch_size = 32, num_subnets = 64, pad_size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 539\u001b[0m\n\u001b[1;32m    535\u001b[0m     inited \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    538\u001b[0m nbhd_img \u001b[38;5;241m=\u001b[39m nbhd_img_orig\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 539\u001b[0m mseg_avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlist_in_atlas\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    540\u001b[0m aseg_avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39marray(alist_in_atlas)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    542\u001b[0m mlist_ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(np\u001b[38;5;241m.\u001b[39marray(mlist_in_atlas), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import socket, os\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers as KL\n",
    "from tqdm import tqdm\n",
    "import glob, copy\n",
    "import scipy\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# from freesurfer import deeplearn as fsd\n",
    "import freesurfer as fs\n",
    "\n",
    "import neurite as ne\n",
    "import neurite_sandbox as nes\n",
    "import voxelmorph as vxm\n",
    "import voxelmorph_sandbox as vxms\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import surfa as sf\n",
    "import generators as gens\n",
    "\n",
    "import layer_dict as ld\n",
    "import pdb as gdb\n",
    "\n",
    "import neurite as ne\n",
    "import neurite_sandbox as nes\n",
    "import voxelmorph as vxm\n",
    "import voxelmorph_sandbox as vxms\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import surfa as sf\n",
    "from utils import *\n",
    "import argparse\n",
    "import csv\n",
    "import random\n",
    "# parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "# parser.add_argument('-lr','--learning_rate',type=float, default=0.0001, help=\"learning rate\")\n",
    "# parser.add_argument('-ie','--initial_epoch',type=int,default=0,help=\"initial epoch\")\n",
    "# parser.add_argument('-b','--batch_size',default=1,type=int,help=\"initial epoch\")\n",
    "# parser.add_argument('-n','--num_subnets',default=16,type=int,help=\"number of subnets\")\n",
    "# parser.add_argument('-e', '--encoder_layers', nargs='+', type=int, help=\"A list of dimensions for the encoder\")\n",
    "# parser.add_argument('-d', '--decoder_layers', nargs='+', type=int, help=\"A list of dimensions for the decoder\")\n",
    "# parser.add_argument('-na', '--no_atlas', action='store_true', default=False, help=\"Custom: Intensities\")\n",
    "# parser.add_argument('-val', '--val', action='store_true', default=False, help=\"feta\")\n",
    "# parser.add_argument('-dataset', '--dataset', choices=['OASIS','Buckner40', 'FBirn','Neurite'], default='OASIS')\n",
    "# parser.add_argument('-m', '--measure', choices=['precision','recall','dice'], default='dice')\n",
    "\n",
    "\n",
    "log_dir = \"logs/train/logs_subnets_new\"\n",
    "models_dir = \"models_subnets_new\"\n",
    "num_epochs = 4000\n",
    "num_subnets = 64\n",
    "\n",
    "if num_subnets:\n",
    "    log_dir += '_'+str(num_subnets)\n",
    "    models_dir += '_'+str(num_subnets)\n",
    "\n",
    "\n",
    "# if args.dataset == \"OASIS\":\n",
    "#     results_dir = 'results/result_oasis_'+args.measure+'_'+str(args.num_subnets)+'.csv'\n",
    "# elif args.dataset == \"Buckner40\":\n",
    "#     results_dir = 'results/result_buckner40_'+args.measure+'_'+str(args.num_subnets)+'.csv'\n",
    "# elif args.dataset == \"FBirn\":\n",
    "#     results_dir = 'results/result_fbirn_'+args.measure+'_'+str(args.num_subnets)+'.csv'\n",
    "# elif args.dataset == \"Neurite\":\n",
    "#     results_dir = 'results/result_neurite_'+args.measure+'_'+str(args.num_subnets)+'.csv'\n",
    "\n",
    "\n",
    "print(\"OASIS loading .. \")\n",
    "\n",
    "adir = '/autofs/cluster/freesurfer/subjects/atlases/aseg_atlas'\n",
    "mname = 'seg_edited.mgz'\n",
    "vname = 'norm.mgz'\n",
    "sfile = os.path.join(adir, 'scripts', 'subjects.txt')\n",
    "# validation_size=20\n",
    "\n",
    "with open(sfile, 'r') as f:\n",
    "    man_subjects = f.read().split('\\n')[0:-1]\n",
    "# man_subjects=man_subjects[0:validation_size]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "read_cached = True # change to true\n",
    "# else:\n",
    "#     log_dir+='/train'\n",
    "odir = '/autofs/vast/braindev/braindev/OASIS/OASIS1/synth-high-res/recon_subject'\n",
    "subjects = [f for f in Path(odir).iterdir() if 'OASIS_OAS1_0' in str(f)]\n",
    "seg_files = [f/f'mri/aseg.mgz' for f in tqdm(subjects)]\n",
    "if not read_cached:\n",
    "    sfile = os.path.join(odir, 'subjects.txt')\n",
    "    with open(sfile, 'r') as f:\n",
    "        man_subjects = f.read().split('\\n')[0:-1]\n",
    "    man_subjects = man_subjects[1:100]\n",
    "    mname = 'aseg.mgz'\n",
    "    vname = 'norm.mgz'\n",
    "    adir = odir\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "# weights_saver = PeriodicWeightsSaver(filepath=models_dir, save_freq=10)  # Save weights every 100 epochs\n",
    "\n",
    "\n",
    "import os, shutil, glob\n",
    "\n",
    "if num_subnets ==8:\n",
    "    aseg_train_fscale = 1.75\n",
    "elif num_subnets == 16 :\n",
    "    aseg_train_fscale = 1.6\n",
    "elif num_subnets == 32 :\n",
    "    aseg_train_fscale = 1.45\n",
    "elif num_subnets == 48 :\n",
    "    aseg_train_fscale = 1.3\n",
    "elif num_subnets == 64 :\n",
    "    aseg_train_fscale = 1.5\n",
    "\n",
    "latest_weight = max(glob.glob(os.path.join(models_dir, 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "latest_epoch = 0\n",
    "if latest_weight:\n",
    "    latest_epoch = int(latest_weight.split('_')[-1].split('.')[0])\n",
    "else:\n",
    "    latest_weight =os.path.join(models_dir, 'weights_epoch_0.h5')\n",
    "    \n",
    "checkpoint_path = latest_weight\n",
    "\n",
    "\n",
    "weights_saver = PeriodicWeightsSaver(filepath=models_dir, latest_epoch=latest_epoch, save_freq=10)  # Save weights every 100 epochs\n",
    "\n",
    "mse_wt = 1\n",
    "\n",
    "vscale = 2\n",
    "vscale = 1\n",
    "\n",
    "dofit = False\n",
    "dofit = True\n",
    "t1=False\n",
    "\n",
    "# whether or not to train the synthseg net from scratch with the subnets\n",
    "# combined_training = True\n",
    "combined_training = False\n",
    "\n",
    "# whether or not to concat the (extracted) aseg features into the subnets\n",
    "concat_aseg = False\n",
    "concat_aseg = True\n",
    "\n",
    "# have subnets only output the labels that ever occur in their location or all labels\n",
    "use_lab2ind = True\n",
    "use_lab2ind = False\n",
    "\n",
    "# put losses on the individual subnets or not\n",
    "use_subloss = False\n",
    "# use_subloss = True\n",
    "\n",
    "# which optimizer to use\n",
    "which_opt = 'sgd'\n",
    "which_opt = 'adam'\n",
    "\n",
    "# which subnet loss to use (if using a subnet loss)\n",
    "subloss = 'mse'\n",
    "subloss = 'dice'\n",
    "\n",
    "# whether to use the insertion code on the subnet outputs (you always should, wasn't sure at first)\n",
    "use_insertion = False\n",
    "use_insertion = True\n",
    "\n",
    "# whether to freeze the aseg weights when training the subnets\n",
    "train_aseg = False\n",
    "train_aseg = True\n",
    "\n",
    "which_loss = 'both'\n",
    "which_loss = 'mse'\n",
    "which_loss = 'cce'\n",
    "which_loss = 'dice'\n",
    "\n",
    "same_contrast=False\n",
    "same_contrast=True\n",
    "\n",
    "# add synthetic outputs shapes to the images or not\n",
    "oshapes = False\n",
    "oshapes = True\n",
    "\n",
    "# perform linear fitting on inputs to softmax to initialize things\n",
    "fit_lin = True\n",
    "fit_lin = False\n",
    "\n",
    "\n",
    "# do affine augmentation or not\n",
    "doaff = False\n",
    "doaff = True\n",
    "\n",
    "\n",
    "# model_dir = 'models'\n",
    "gpuid = -1\n",
    "host = socket.gethostname()\n",
    "from neurite_sandbox.tf.utils.utils import plot_fit_callback as pfc\n",
    "\n",
    "\n",
    "print(f'host name {socket.gethostname()}')\n",
    "\n",
    "# ngpus = 1 if os.getenv('NGPUS') is None else int(os.getenv('NGPUS'))\n",
    "\n",
    "ngpus =0 #len(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "gpu_ids =0 #os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n",
    "\n",
    "print(f'using {ngpus} gpus')\n",
    "if ngpus > 1:\n",
    "    model_device = '/gpu:0'\n",
    "    synth_device = '/gpu:1'\n",
    "    synth_gpu  = 1\n",
    "    val_device=2\n",
    "    dev_str = \"0, 1\"\n",
    "    # dev_str = \"0, 1, 2\"\n",
    "    print(\"dev_str:\",dev_str)\n",
    "else:\n",
    "    model_device = '/gpu:0'\n",
    "    synth_device = model_device\n",
    "    synth_gpu = 0\n",
    "    dev_str = \"0\"\n",
    "    val_device=0\n",
    "\n",
    "if not dofit and host == 'serena.nmr.mgh.harvard.edu':\n",
    "    dev_str = '/cpu:0'\n",
    "    print(f'setting dev_str to {dev_str}')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"0,1,2,3\"\n",
    "\n",
    "print(f'model_device {model_device}, synth_device {synth_device}, dev_str {dev_str}')\n",
    "\n",
    "print(f'physical GPU # is {os.getenv(\"SLURM_STEP_GPUS\")}')\n",
    "ret = ne.utils.setup_device(dev_str)\n",
    "# policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "\n",
    "print(f'dofit {dofit}, doaff {doaff}, fit_lin {fit_lin}, oshapes {oshapes}, subnet loss {use_subloss}, use_lab2ind {use_lab2ind}, combined_training {combined_training}, insertion {use_insertion}, concat aseg {concat_aseg}, subloss {subloss}')\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "\n",
    "if dofit:\n",
    "    print(f'TRAINING model with loss {which_loss}')\n",
    "else:\n",
    "    print(f'loading model trained with {which_loss} loss')\n",
    "\n",
    "target_shape = (192,)*3\n",
    "inshape = target_shape\n",
    "\n",
    "\n",
    "\n",
    "# number of subnets and the size of the input patch to each one\n",
    "patch_size = 32\n",
    "num_subnets = num_subnets\n",
    "\n",
    "initial_epoch = 0\n",
    "\n",
    "pad_size=4\n",
    "# add some padding to the subnets so that there is always some context for each voxel\n",
    "big_patch_size = patch_size + 2 * pad_size\n",
    "phalf = patch_size // 2\n",
    "bphalf = big_patch_size // 2\n",
    "\n",
    "print(f'patch_size = {patch_size}, num_subnets = {num_subnets}, pad_size = {pad_size}')\n",
    "\n",
    "ntest = 25\n",
    "crop = -1 if dofit else ntest\n",
    "crop = 20\n",
    "validation_size=20\n",
    "crop_indices = np.random.choice(len(seg_files), crop, replace=False)\n",
    "\n",
    "lut = fs.lookups.default()\n",
    "\n",
    "# csf = lut.search('CSF')[0]\n",
    "lesion_label_orig = lut.search('Left-Lesion')\n",
    "if len(lesion_label_orig) > 0:\n",
    "    lesion_label_orig = lesion_label_orig[0]\n",
    "else:   # not in the lut - add a new one\n",
    "    lesion_label_orig = 77\n",
    "    lut.add(lesion_label_orig, 'Lesion', color=[240,240,240])\n",
    "\n",
    "# if 'inited' not in locals() and 'inited' not in globals():\n",
    "    # inited = False\n",
    "inited=False\n",
    "warp_max = 2\n",
    "if 0:\n",
    "    mri_segs_orig = [sf.load_volume(str(seg_files[i])) for i in tqdm(crop_indices)]\n",
    "\n",
    "\n",
    "    if vscale > 1:\n",
    "        print(f'downsampling by {vscale}')\n",
    "        mri_segs = [mri.reslice(vscale) for mri in tqdm(mri_segs_orig)]\n",
    "    else:\n",
    "        print(f'cropping to {target_shape}')\n",
    "        mri_segs = [mri.fit_to_shape(target_shape, center='bbox') for mri in tqdm(mri_segs_orig)]\n",
    "        # mri_segs = [mri.reshape(target_shape, center='bbox') for mri in tqdm(mri_segs)]\n",
    "\n",
    "    np_segs_orig = [mri.data for mri in mri_segs]\n",
    "    fname = 'npy_malte/oasis_labels.npy'\n",
    "    if os.path.exists(fname):\n",
    "        print(f'loading unique labels in {len(mri_segs_orig)} from {fname}')\n",
    "        labels_orig = np.load(fname)\n",
    "    else:\n",
    "        print(f'finding unique labels in {len(mri_segs_orig)} datasets...')\n",
    "        labels_orig = np.unique(np.array(np_segs_orig))\n",
    "        np.save(fname, labels_orig)\n",
    "\n",
    "    ### map csf with bg\n",
    "    # csf_label = target_lut.search('CSF')[0]\n",
    "    np_segs = [vol.data for vol in mri_segs]\n",
    "    np_segs_no_csf = []\n",
    "    for vol in np_segs:\n",
    "        vol_copy = np.array(vol)  # Make a copy to avoid modifying the original\n",
    "        vol_copy[vol_copy == 24] = 0\n",
    "        np_segs_no_csf.append(vol_copy)\n",
    "        \n",
    "    # mapping = fs.lookups.tissue_type_recoder_no_skull(include_lesions=use_lesions)\n",
    "    mapping = fs.lookups.nonlateral_aseg_recoder()\n",
    "    target_lut = mapping.target_lut\n",
    "    lut_name = 'nonlat.txt'\n",
    "    mri_segs_recoded = [fs.label.recode(mri, mapping) for mri in np_segs_no_csf]  \n",
    "    lesion_label = target_lut.search('Left-Lesion')[0]\n",
    "\n",
    "    np_segs = [vol.data for vol in mri_segs_recoded]\n",
    "    labels_in = np.unique(np.array(np_segs)).astype(int)\n",
    "        \n",
    "    \n",
    "    # labels_in = np.unique(np.array(np_segs_no_csf)).astype(int)\n",
    "    if lesion_label not in labels_in:\n",
    "        l = list(labels_in)\n",
    "        l.append(lesion_label)\n",
    "        labels_in = np.array(l)\n",
    "    nlabels_small = len(labels_in)\n",
    "    label_map = {}\n",
    "    keys = mapping.mapping.keys()\n",
    "    lab_to_ind = np.zeros((labels_orig.max()+1,), dtype=int)\n",
    "    for label in labels_orig:\n",
    "        if label not in keys:\n",
    "            output_label = 0\n",
    "        else:\n",
    "            output_label = mapping.mapping[label]\n",
    "        label_map[label] = output_label\n",
    "        lab_to_ind[label] = output_label\n",
    "\n",
    "\n",
    "    import generators as gens\n",
    "    vxm_model = gens.read_vxm_model(inshape)\n",
    "    vxm_smooth_wt = np.zeros((1, 1))\n",
    "    vxm_smooth_wt[0,0] = .3   # warp regularization hyper parameter\n",
    "    \n",
    "    gen_model = gens.create_gen_model(mri_segs_recoded, oshapes, synth_device, nlabels_small, labels_in, inshape, warp_max)\n",
    "\n",
    "    mapping = fs.lookups.nonlateral_aseg_recoder()\n",
    "    target_lut = mapping.target_lut\n",
    "    lut_name = 'nonlat.txt'\n",
    "\n",
    "    mri_man_segs = []  # manual segs\n",
    "    mri_norms = []  # mri vols\n",
    "    mri_norms_orig = []\n",
    "    mri_man_segs_orig = []\n",
    "\n",
    "    for s in tqdm(man_subjects):\n",
    "        # mri_seg_orig = fs.Volume.read(os.path.join(adir, s, 'mri', mname))\n",
    "        mri_seg_orig = sf.load_volume(os.path.join(adir, s, 'mri', mname))\n",
    "\n",
    "    \n",
    "        mri_man_segs_orig.append(mri_seg_orig)\n",
    "        # mri_seg = mri_seg_orig.fit_to_shape(target_shape, center='bbox')\n",
    "        mri_seg = mri_seg_orig.reshape(target_shape)\n",
    "        mri_man_segs.append(mri_seg)\n",
    "        # mri_norm_orig = fs.Volume.read(os.path.join(adir, s, 'mri', vname))\n",
    "        mri_norm_orig = sf.load_volume(os.path.join(adir, s, 'mri', vname))\n",
    "\n",
    "        mri_norm = mri_norm_orig.resample_like(mri_seg)\n",
    "        mri_norms.append(mri_norm)\n",
    "        mri_norms_orig.append(mri_norm_orig)\n",
    "\n",
    "    mri_man_segs_recoded = [fs.label.recode(mri, mapping) for mri in tqdm(mri_man_segs)]\n",
    "\n",
    "\n",
    "    if 0:\n",
    "        s_moved = np.concatenate(moved, axis=0)\n",
    "        mri_seg_atlas = sf.Volume(np.mean(s_moved, axis=0))\n",
    "        mri_seg_atlas.data = unify_left_right(mri_seg_atlas.data)\n",
    "    else:\n",
    "        mri_seg_atlas = sf.load_volume(\"atlas_100_onehot.mgz\").reshape(target_shape)\n",
    "        mri_seg_atlas.data = unify_left_right(mri_seg_atlas.data)\n",
    "\n",
    "        num_classes = len(np.unique(mri_seg_atlas.data))\n",
    "        one_hot_data = tf.one_hot(mri_seg_atlas.data, num_classes)\n",
    "        mri_seg_atlas.data = one_hot_data\n",
    "    \n",
    "    hard_seg = np.argmax(mri_seg_atlas.data, axis=-1)\n",
    "    mri_hard_seg = mri_seg_atlas.copy()\n",
    "    mri_hard_seg.data = hard_seg\n",
    "    mri_hard_seg_cropped = mri_hard_seg.reshape(target_shape)\n",
    "    print(mri_hard_seg_cropped.shape)\n",
    "    # mri_norm_atlas = sf.load_volume(\"norm_atlas.mgz\").resample_like(mri_hard_seg)\n",
    "    mri_norm_atlas = sf.load_volume(\"atlas_100.mgz\").resample_like(mri_hard_seg)\n",
    "\n",
    "    print(mri_norm_atlas.shape)\n",
    "    # mri_hard_seg = mri_seg_atlas.copy(hard_seg).fit_to_shape(target_shape, center='bbox')\n",
    "    # mri_norm_atlas = fs.Volume.read(\"norm_atlas.mgz\").resample_like(mri_hard_seg)\n",
    "\n",
    "    mri_seg_atlas = mri_seg_atlas.resample_like(mri_hard_seg)\n",
    "    norm_atlas = (mri_norm_atlas.data / mri_norm_atlas.data.max())[np.newaxis, ..., np.newaxis]\n",
    "    print(\"norm atlas shape: \", norm_atlas.shape,mri_hard_seg.shape,mri_seg_atlas.shape)\n",
    "    \n",
    "    f = 128\n",
    "\n",
    "    conf = {\n",
    "       'def.enc_nf': [f] * 4,\n",
    "       'def.dec_nf': [f] * 4,\n",
    "       'def.add_nf': [f] * 4,\n",
    "       'def.hyp_den': [32] * 4,\n",
    "    }\n",
    "\n",
    "    vxm_model = vxms.networks.VxmJointAverage(in_shape=inshape, **conf)\n",
    "    vxm_model.load_weights(os.path.join('models_from_Malte', f'VxmJointAverage{f}.h5'))\n",
    "    # vxm_model = vxm.networks.HyperVxmJoint(in_shape=inshape, **conf)\n",
    "    # vxm_model.load_weights(os.path.join('models_from_Malte', f'hyp_mse_uni_{f}_lm10_mid.h5'))\n",
    "    #aseg_model = tf.keras.models.load_model('aseg.h5', custom_objects=ld.layer_dict)\n",
    "\n",
    "    # aseg_train_fscale = 1.75\n",
    "    if 1:\n",
    "        aseg_train_fname = f'aseg_subnet/aseg.fscale.{aseg_train_fscale}.h5'\n",
    "        print(f'loading aseg model {aseg_train_fname}')\n",
    "        aseg_model = tf.keras.models.load_model(aseg_train_fname, \n",
    "                                                custom_objects=ld.layer_dict)\n",
    "    else:\n",
    "        aseg_model = tf.keras.models.load_model('aseg.h5', custom_objects=ld.layer_dict)\n",
    "\n",
    "    l = np.zeros((1, 1))\n",
    "    l[0,0] = .3   # warp regularization hyper parameter\n",
    "\n",
    "    lfunc = nes.losses.DiceNonzero(nlabels_small, weights=None, check_input_limits=False).loss\n",
    "    # read_cached = False\n",
    "    \n",
    "    new_cache = True  # change to True\n",
    "    if not read_cached:\n",
    "        dice_list = []\n",
    "        elist = []\n",
    "        elist_in_atlas = []\n",
    "        alist_in_atlas = []\n",
    "        nlist_in_atlas = []\n",
    "        mlist_in_atlas = []\n",
    "        # man_subjects\n",
    "        for sno, s in enumerate(tqdm(man_subjects)):\n",
    "            print(\"%%%%%%%%%%%\",mri_man_segs_recoded[sno].shape,sno)\n",
    "            mseg_onehot = np.eye(nlabels_small)[mri_man_segs_recoded[sno].data]\n",
    "            norm = (mri_norms[sno].data / mri_norms[sno].data.max())[np.newaxis, ..., np.newaxis]\n",
    "            pred = aseg_model.predict(norm)\n",
    "            transform = vxm_model.predict([l, norm, norm_atlas])\n",
    "            dice = lfunc(tf.convert_to_tensor(mseg_onehot[np.newaxis], tf.float32), \n",
    "                         tf.convert_to_tensor(pred, tf.float32))\n",
    "            dice_list.append(dice.numpy())\n",
    "            ev = (mseg_onehot - pred[0])\n",
    "            evol = (ev**2).sum(axis=-1)\n",
    "            elist.append(evol)\n",
    "            evol_in_atlas = vxm.layers.SpatialTransformer(interp_method='linear', fill_value=0)([evol[np.newaxis, ..., np.newaxis], transform])\n",
    "            norm_in_atlas = vxm.layers.SpatialTransformer(interp_method='linear', fill_value=0)([norm, transform])\n",
    "            aseg_in_atlas = vxm.layers.SpatialTransformer(interp_method='linear', fill_value=0)([pred[0][np.newaxis, ...], transform])\n",
    "            mseg_in_atlas = vxm.layers.SpatialTransformer(interp_method='linear', fill_value=0)([mseg_onehot[np.newaxis, ...], transform])\n",
    "            elist_in_atlas.append(evol_in_atlas.numpy().squeeze())\n",
    "            nlist_in_atlas.append(norm_in_atlas.numpy().squeeze())\n",
    "            alist_in_atlas.append(aseg_in_atlas.numpy().squeeze())\n",
    "            mlist_in_atlas.append(mseg_in_atlas.numpy().squeeze())\n",
    "\n",
    "        np.save(f'npy_malte/elist_in_atlas.fscale.{aseg_train_fscale}.npy', elist_in_atlas)\n",
    "        np.save(f'npy_malte/nlist_in_atlas.fscale.{aseg_train_fscale}.npy', nlist_in_atlas)\n",
    "        np.save(f'npy_malte/alist_in_atlas.fscale.{aseg_train_fscale}.npy', alist_in_atlas)\n",
    "        np.save(f'npy_malte/mlist_in_atlas.fscale.{aseg_train_fscale}.npy', mlist_in_atlas)\n",
    "\n",
    "        print('computing index occurence volumes')\n",
    "        aseg_inds = np.argmax(np.array(alist_in_atlas), axis=-1)\n",
    "        mseg_inds = np.argmax(np.array(mlist_in_atlas), axis=-1)\n",
    "        max_inds = nlabels_small\n",
    "        mseg_ind_vol = np.zeros(target_shape + (max_inds,))\n",
    "        aseg_ind_vol = np.zeros(target_shape + (max_inds,))\n",
    "        mseg_num_inds = np.zeros(target_shape)\n",
    "        aseg_num_inds = np.zeros(target_shape)\n",
    "        for x in tqdm(range(aseg_inds.shape[1])):\n",
    "            for y in range(aseg_inds.shape[2]):\n",
    "                for z in range(aseg_inds.shape[3]):\n",
    "                    ind_list = -1 * np.ones((max_inds,))\n",
    "                    u = np.unique(mseg_inds[:, x, y, z])\n",
    "                    for lno, l in enumerate(u):\n",
    "                        if lno >= max_inds:\n",
    "                            break\n",
    "                            mseg_ind_vol[x, y, z, lno] = l\n",
    "                \n",
    "                    mseg_num_inds[x, y, z] = len(u)\n",
    "                    ind_list = -1 * np.ones((max_inds,))\n",
    "                    u = np.unique(aseg_inds[:, x, y, z])\n",
    "                    for lno, l in enumerate(u):\n",
    "                        aseg_ind_vol[x, y, z, lno] = l\n",
    "\n",
    "                    aseg_num_inds[x, y, z] = len(u)\n",
    "\n",
    "        inds_trivial = np.nonzero(mseg_num_inds < 2)\n",
    "        evol_avg = np.array(elist_in_atlas).mean(axis=0)\n",
    "        evol_avg[inds_trivial] = 0\n",
    "        nvol_avg = np.array(nlist_in_atlas).mean(axis=0)\n",
    "        low_val = 0 if num_subnets == 20 else -1000\n",
    "        nbhd_img_orig = scipy.ndimage.convolve(evol_avg, np.ones((patch_size,)*3)/(patch_size**3), \n",
    "                                               mode='constant',  cval=low_val)\n",
    "        np.save(f'npy_malte/nbhd_img.fscale.{aseg_train_fscale}.npy', nbhd_img_orig)\n",
    "        np.save(f'npy_malte/evol_avg.fscale.{aseg_train_fscale}.npy', evol_avg)\n",
    "        np.save(f'npy_malte/nvol_avg.fscale.{aseg_train_fscale}.npy', nvol_avg)\n",
    "    else:   # not dofit\n",
    "        if new_cache:\n",
    "            print(f'reading cached volumes scale {aseg_train_fscale}')\n",
    "            nbhd_img_orig = np.load('npy_malte/nbhd_img.npy', allow_pickle=True)\n",
    "            elist_in_atlas = np.load(f'npy_malte/elist_in_atlas.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "            nlist_in_atlas = np.load(f'npy_malte/nlist_in_atlas.fscale.{aseg_train_fscale}.npy', allow_pickle=True) \n",
    "            alist_in_atlas = np.load(f'npy_malte/alist_in_atlas.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "            mlist_in_atlas = np.load(f'npy_malte/mlist_in_atlas.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "            evol_avg = np.load(f'npy_malte/evol_avg.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "            nvol_avg = np.load(f'npy_malte/nvol_avg.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "        else:\n",
    "            print('reading cached volumes')\n",
    "            nbhd_img_orig = np.load('npy_malte/nbhd_img.npy', allow_pickle=True)\n",
    "            elist_in_atlas = np.load('npy_malte/elist_in_atlas.npy', allow_pickle=True)\n",
    "            nlist_in_atlas = np.load('npy_malte/nlist_in_atlas.npy', allow_pickle=True)\n",
    "            alist_in_atlas = np.load('npy_malte/alist_in_atlas.npy', allow_pickle=True)\n",
    "            mlist_in_atlas = np.load('npy_malte/mlist_in_atlas.npy', allow_pickle=True)\n",
    "            evol_avg = np.load('npy_malte/evol_avg.npy', allow_pickle=True)\n",
    "            nvol_avg = np.load('npy_malte/nvol_avg.npy', allow_pickle=True)\n",
    "\n",
    "    inited = True\n",
    "\n",
    "\n",
    "nbhd_img = nbhd_img_orig.copy()\n",
    "mseg_avg = np.argmax(np.array(mlist_in_atlas).mean(axis=0), axis=-1)\n",
    "aseg_avg = np.argmax(np.array(alist_in_atlas).mean(axis=0), axis=-1)\n",
    "    \n",
    "mlist_ind = np.argmax(np.array(mlist_in_atlas), axis=-1)\n",
    "alist_ind = np.argmax(np.array(alist_in_atlas), axis=-1)\n",
    "\n",
    "\n",
    "if dofit:    #  run training\n",
    "    patch_labels = []\n",
    "    patch_centers = []\n",
    "    for netno in range(num_subnets):\n",
    "        max_ind = np.argmax(nbhd_img[bphalf:-bphalf, bphalf:-bphalf, bphalf:-bphalf])\n",
    "        patch_center = np.unravel_index(max_ind, nbhd_img[bphalf:-bphalf, bphalf:-bphalf, bphalf:-bphalf].shape) + np.array([bphalf, bphalf, bphalf])\n",
    "        # should use bphalf here\n",
    "        x0 = patch_center[0]-patch_size//2\n",
    "        x1 = x0 + patch_size \n",
    "        y0 = patch_center[1]-patch_size//2\n",
    "        y1 = y0 + patch_size \n",
    "        z0 = patch_center[2]-patch_size//2\n",
    "        z1 = z0 + patch_size\n",
    "        labels_mseg = np.unique(mlist_ind[:, x0:x1, y0:y1, z0:z1])\n",
    "        labels_aseg = np.unique(alist_ind[:, x0:x1, y0:y1, z0:z1])\n",
    "        print(f'patch at {patch_center}: labels {labels_mseg}')\n",
    "        patch_centers.append(patch_center)\n",
    "        bx0 = patch_center[0]-big_patch_size//2\n",
    "        bx1 = x0 + big_patch_size \n",
    "        by0 = patch_center[1]-big_patch_size//2\n",
    "        by1 = y0 + big_patch_size \n",
    "        bz0 = patch_center[2]-big_patch_size//2\n",
    "        bz1 = z0 + big_patch_size\n",
    "        nbhd_img[bx0:bx1, by0:by1, bz0:bz1] = -1000  # prevent overlap of subsequent patches\n",
    "        print(\"patch label shape:\",labels_mseg.shape)\n",
    "        patch_labels.append(labels_mseg)\n",
    "\n",
    "    print('saving patch centers and labels...')\n",
    "    np.savez(f'npy_malte/patch_labels.{patch_size}.{num_subnets}.{pad_size}.npy', *patch_labels)\n",
    "    np.save(f'npy_malte/patch_centers.{patch_size}.{num_subnets}.{pad_size}.npy', patch_centers)\n",
    "else:\n",
    "    print('loading patch centers and labels...')\n",
    "    patch_labels_files = np.load(f'npy_malte/patch_labels.{patch_size}.{num_subnets}.{pad_size}.npy', allow_pickle=True)\n",
    "    patch_labels = [patch_labels_files[key] for key in patch_labels_files.files]\n",
    "\n",
    "    patch_centers = np.load(f'npy_malte/patch_centers.{patch_size}.{num_subnets}.{pad_size}.npy', allow_pickle=True)\n",
    "\n",
    "fscale = aseg_train_fscale\n",
    "combined_training = True\n",
    "if combined_training:   # train synthseg net and subnets from scratch\n",
    "    nfeats = 64\n",
    "    unet_nf = []\n",
    "    nb_conv_per_level = 2\n",
    "    \n",
    "    nb_levels = int(np.log2(inshape[0]))-(1)   # 4,4,4 is lowest level\n",
    "\n",
    "    for level in range(nb_levels):\n",
    "        filters_in_this_level = []\n",
    "        for layer in range(nb_conv_per_level):\n",
    "            filters_in_this_level.append(int(fscale**level*nfeats))\n",
    "        \n",
    "        unet_nf.append(filters_in_this_level)\n",
    "\n",
    "    model_lin = ne.models.unet(unet_nf, inshape+(1,), None, 3, nlabels_small, feat_mult=None, final_pred_activation='linear')\n",
    "    softmax_out = KL.Softmax(name='seg')(model_lin.outputs[0])\n",
    "    aseg_model = tf.keras.Model(model_lin.inputs, [softmax_out])\n",
    "else:\n",
    "    aseg_train_fname = f'aseg_subnet/aseg.fscale.{aseg_train_fscale}.h5'\n",
    "    print(f'loading aseg model {aseg_train_fname}')\n",
    "    aseg_model = tf.keras.models.load_model(aseg_train_fname, custom_objects=ld.layer_dict)\n",
    "\n",
    "# find last feature layer of the aseg unet\n",
    "for lno in range(len(aseg_model.layers)-1, 0, -1):\n",
    "    aseg_layer = aseg_model.layers[lno]\n",
    "    if aseg_layer.name.startswith(\"unet_conv_uparm\"):\n",
    "        break\n",
    "\n",
    "nfeats = aseg_layer.output[0].get_shape().as_list()[-1]\n",
    "\n",
    "\n",
    "gen_model = gens.create_gen_model(np_segs, oshapes, synth_device, nlabels_small, labels_in, inshape)\n",
    "\n",
    "\n",
    "# build lists for unet architecture\n",
    "nb_levels = int(np.log2(patch_size))-1\n",
    "nb_levels = int(np.log2(big_patch_size))-1\n",
    "nb_conv_per_level = 2\n",
    "unet_scale = 1\n",
    "    \n",
    "unet_nf = []\n",
    "\n",
    "\n",
    "for level in range(nb_levels):\n",
    "    filters_in_this_level = []\n",
    "    for layer in range(nb_conv_per_level):\n",
    "        filters_in_this_level.append(int(fscale**level*nfeats))\n",
    "        \n",
    "    unet_nf.append(filters_in_this_level)\n",
    "\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "lr = 1e-5\n",
    "lr = 1e-4\n",
    "name = f'subnets.outside.unet_nf.{nfeats}.warp_max.{warp_max}.oshapes.{oshapes}.num_subnets.{num_subnets}.psize.{patch_size}.pad.{pad_size}.lab2ind.{use_lab2ind}.lr.{lr}.subloss.{use_subloss}.insertion.{use_insertion}.combined_training.{combined_training}.train_aseg.{train_aseg}'\n",
    "if not concat_aseg:\n",
    "    name += f'.concat_aseg.{concat_aseg}'\n",
    "if use_subloss and subloss != 'dice':\n",
    "    name += f'.subloss.{subloss}'\n",
    "if which_opt != 'adam':\n",
    "    name += f'.which_opt.{which_opt}'\n",
    "\n",
    "\n",
    "\n",
    "label_weights = np.ones((1,nlabels_small,))\n",
    "# label_weights[0,-1] = .01  # downweight lesion class\n",
    "lfunc = ne.losses.Dice(nb_labels=nlabels_small, weights=None, check_input_limits=False).mean_loss\n",
    "lfunc = nes.losses.DiceNonzero(nlabels_small, weights=None, check_input_limits=False).loss\n",
    "if use_subloss:\n",
    "    if subloss == 'dice':\n",
    "        thresh = -.2*3\n",
    "    else:\n",
    "        thresh = 8\n",
    "else:\n",
    "        thresh = -.2*2\n",
    "    \n",
    "\n",
    "cooldown = 25\n",
    "patience = 600\n",
    "\n",
    "\n",
    "losses = [lfunc]\n",
    "loss_weights = [1]\n",
    "if use_subloss:\n",
    "    if subloss == 'mse':\n",
    "        lfunc_subnet = lambda a, b: mse_wt * tf.keras.losses.MSE(a, b)\n",
    "    else:\n",
    "        lfunc_subnet = lfunc\n",
    "    #lfunc_mse = tf.keras.losses.mse\n",
    "    losses += [lfunc_subnet]\n",
    "    loss_weights += [1]\n",
    "\n",
    "\n",
    "unet_device = model_device if (fit_lin or dofit) else synth_device\n",
    "with tf.device(unet_device):     # add the subnets to the big unet\n",
    "    aseg_shape = aseg_layer.output.get_shape().as_list()[1:]\n",
    "    aseg_linear_out = aseg_model.layers[-3].output  # tensor right before outputs compressed to nlabels\n",
    "\n",
    "    subnet_outputs_to_add = [aseg_linear_out]  # add subnet outputs to this tensor for final output\n",
    "    subnet_outputs = []  # list of subnet outputs each nlabels_small long (only for using subloss)\n",
    "    subnet_patches = []  # spatial location of the subnet patches\n",
    "    pre_lab2ind = []\n",
    "\n",
    "    for subnet_no in range(num_subnets):\n",
    "        bx0 = patch_centers[subnet_no][0]-big_patch_size//2\n",
    "        bx1 = bx0 + big_patch_size \n",
    "        by0 = patch_centers[subnet_no][1]-big_patch_size//2\n",
    "        by1 = by0 + big_patch_size \n",
    "        bz0 = patch_centers[subnet_no][2]-big_patch_size//2\n",
    "        bz1 = bz0 + big_patch_size \n",
    "        # only inner part without padding\n",
    "        x0 = patch_centers[subnet_no][0]-patch_size//2\n",
    "        x1 = x0 + patch_size \n",
    "        y0 = patch_centers[subnet_no][1]-patch_size//2\n",
    "        y1 = y0 + patch_size \n",
    "        z0 = patch_centers[subnet_no][2]-patch_size//2\n",
    "        z1 = z0 + patch_size \n",
    "        # labels_mseg = np.unique(mlist_ind[:, bx0:bx1, by0:by1, bz0:bz1])\n",
    "        labels_mseg = np.unique(mlist_ind[:, x0:x1, y0:y1, z0:z1])\n",
    "        noutputs = len(labels_mseg) if use_lab2ind else nlabels_small\n",
    "\n",
    "        # extract a patch of (1) the data on the uparm of the big net and (2) the input volume\n",
    "        aseg_patch = nes.layers.ExtractPatch(((bx0, bx1), (by0, by1), (bz0, bz1)), \n",
    "                                             name=f'aseg_patch{subnet_no}')(aseg_layer.output)\n",
    "        patch_input = nes.layers.ExtractPatch(((bx0, bx1), (by0, by1), (bz0, bz1)),\n",
    "                                              name=f'subnet_input{subnet_no}')(aseg_model.inputs[0])\n",
    "        nf = aseg_patch.get_shape().as_list()[-1]\n",
    "        print(\"unet, big_patch_size, noutputs: \",unet_nf,big_patch_size,noutputs)\n",
    "        subnet_lin = ne.models.unet(unet_nf, (big_patch_size,)*3+(1,), None, 3, noutputs, feat_mult=None, final_pred_activation='linear', name=f'subnet{subnet_no}')\n",
    "\n",
    "        if use_lab2ind:\n",
    "            # concat aseg_model info to second-to-last layer of subnet\n",
    "            if concat_aseg:\n",
    "                tmp_model = tf.keras.Model(subnet_lin.inputs, subnet_lin.layers[-3].output)\n",
    "                tmp_out = tmp_model(patch_input)\n",
    "                unet_concat = KL.Concatenate(name=f'subnet_in{subnet_no}', axis=-1)([tmp_out, aseg_patch])\n",
    "                Conv = getattr(KL, 'Conv%dD' % 3)\n",
    "                subnet_out = Conv(noutputs, 3, strides=1, padding='same')(unet_concat)\n",
    "            else:\n",
    "                subnet_out = subnet_lin(patch_input)\n",
    "            \n",
    "            pre_lab2ind.append(subnet_out)\n",
    "            unet_out = nes.layers.IndexToLabel(labels_mseg, nlabels_small, name=f'IndToLab{subnet_no}')(subnet_out)\n",
    "        else:  # what about concatting in the non lab2ind case????\n",
    "            unet_out = subnet_lin(patch_input)\n",
    "\n",
    "        # crop out the beginning and ending pad regions so output is just central patch_size\n",
    "        p0 = pad_size\n",
    "        p1 = pad_size+patch_size\n",
    "        patch_output = nes.layers.ExtractPatch(((p0, p1), (p0, p1), (p0, p1)),\n",
    "                                               name=f'subnet_output{subnet_no}')(unet_out)\n",
    "\n",
    "        if use_subloss:\n",
    "            if subloss == 'dice':\n",
    "                subnet_softmax = KL.Softmax(name=f'subnet{subnet_no}_softmax')(patch_output)\n",
    "                subnet_outputs.append(subnet_softmax[:, tf.newaxis, ...])\n",
    "            else:   # just include the linear output for mse loss\n",
    "                subnet_outputs.append(patch_output[:, tf.newaxis, ...])\n",
    "\n",
    "            subnet_patches.append(((x0, x1), (y0, y1), (z0, z1)))\n",
    "\n",
    "        if subnet_no == 0 or not use_insertion:\n",
    "            # padding = ((x0, aseg_shape[0]-x1), (y0, aseg_shape[1]-y1), (z0, aseg_shape[2]-z1))\n",
    "            # padded_unet_output = nes.layers.Pad(padding=padding, mode='constant', name=f'pad_subnet{subnet_no}')(patch_output)\n",
    "            from keras.layers import ZeroPadding3D\n",
    "\n",
    "            # Assuming 'patch_output' is a Keras tensor\n",
    "            padding = ((x0, aseg_shape[0] - x1), (y0, aseg_shape[1] - y1), (z0, aseg_shape[2] - z1))\n",
    "            padded_unet_output = ZeroPadding3D(padding=padding, name=f'pad_subnet{subnet_no}')(patch_output)\n",
    "\n",
    "            if not use_insertion:\n",
    "                subnet_outputs_to_add.append(padded_unet_output)\n",
    "            else:   # initialize one big tensor with patch outputs\n",
    "                big_patches_output = padded_unet_output\n",
    "        else:  # just insert this patch (since they don't overlap don't need to add)\n",
    "            offset = ((x0, y0, z0, 0))\n",
    "            big_patches_output = nes.layers.InsertPatch(big_patches_output, offset, \n",
    "                                                         name=f'patch_insert{subnet_no}')([\n",
    "                                                             patch_output, big_patches_output])\n",
    "            \n",
    "\n",
    "    if not use_insertion:\n",
    "        summed_aseg_outputs = KL.Add(name='patch_plus_unet')(subnet_outputs_to_add)\n",
    "    else:\n",
    "        summed_patch_outputs = KL.Lambda(lambda x: x, name='summed_patch_outputs')(big_patches_output)\n",
    "        summed_aseg_outputs = KL.Add(name='patch_plus_unet')([summed_patch_outputs, aseg_linear_out])\n",
    "\n",
    "    model_lin = tf.keras.Model(aseg_model.inputs, [summed_aseg_outputs])\n",
    "    softmax_out = KL.Softmax(name='seg')(summed_aseg_outputs)\n",
    "    outputs = [softmax_out]\n",
    "    if use_subloss:\n",
    "        subnet_out = KL.Concatenate(name='subloss', axis=1)(subnet_outputs)\n",
    "        outputs += [subnet_out]\n",
    "    model = tf.keras.Model(aseg_model.inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03cb85f3-85e9-49eb-8af1-5edd01072b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 192, 192)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0551ba6-5bae-44b6-8256-c3d950554b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.,   0.,   0.,  96.],\n",
       "       [  0.,   0.,   1., -96.],\n",
       "       [  0.,  -1.,   0.,  96.],\n",
       "       [  0.,   0.,   0.,   1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(trans_orig[1].geom.vox2world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5943bbdb-baed-4d44-b0e6-09a98e983a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_files = sorted(trans_files, key=extract_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "750378c6-1e4b-4448-b368-724ca8665b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_seg = np.argmax(mri_seg_atlas.data, axis=-1).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93c10256-4c55-4872-9d1e-0efd99f4d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.save(nib.Nifti1Image(hard_seg,np.array(trans_orig[1].geom.vox2world)), f\"atlas_1h.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "512b9b1c-faeb-4e6e-9374-f337f17f29f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/41 [00:00<00:17,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 41 42 43 44\n",
      " 46 47 49 50 51 52 53 54 58 60 62 78 79 81 82 85 86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/41 [00:00<00:19,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 77]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/41 [00:01<00:19,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 77 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4/41 [00:01<00:18,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 41 42 43 44\n",
      " 46 47 49 50 51 52 53 54 58 60 62 72 85 86]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5/41 [00:02<00:17,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 77 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 6/41 [00:02<00:17,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 77 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 7/41 [00:03<00:17,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 77 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 8/41 [00:04<00:16,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 9/41 [00:04<00:16,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 10/41 [00:05<00:15,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 11/41 [00:05<00:15,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 12/41 [00:06<00:15,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 13/41 [00:06<00:14,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 77 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 14/41 [00:07<00:13,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 77 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 15/41 [00:07<00:13,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 77 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 16/41 [00:08<00:12,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 17/41 [00:08<00:12,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 18/41 [00:09<00:11,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 77 80 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 19/41 [00:09<00:11,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  4  5  7  8 10 11 12 13 14 15 16 17 18 24 26 28 30 31 41 42 43\n",
      " 44 46 47 49 50 51 52 53 54 58 60 62 63 72 77 80 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 19/41 [00:10<00:11,  1.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm(man_subjects):\n\u001b[1;32m     22\u001b[0m     mri_seg_orig \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mload_volume(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(adir, s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmri\u001b[39m\u001b[38;5;124m'\u001b[39m, mname))\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmri_seg_orig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m     mri_man_segs_orig\u001b[38;5;241m.\u001b[39mappend(mri_seg_orig)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# break\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import surfa as sf\n",
    "import neurite as ne\n",
    "import neurite_sandbox as nes\n",
    "import voxelmorph as vxm\n",
    "import voxelmorph_sandbox as vxms\n",
    "import re\n",
    "from utils import *\n",
    "\n",
    "adir = '/autofs/cluster/freesurfer/subjects/atlases/aseg_atlas'\n",
    "mname = 'seg_edited.mgz'\n",
    "vname = 'norm.mgz'\n",
    "sfile = os.path.join(adir, 'scripts', 'subjects.txt')\n",
    "validation_size=20\n",
    "\n",
    "with open(sfile, 'r') as f:\n",
    "    man_subjects = f.read().split('\\n')[0:-1]\n",
    "mri_man_segs_orig = []\n",
    "\n",
    "for s in tqdm(man_subjects):\n",
    "    mri_seg_orig = sf.load_volume(os.path.join(adir, s, 'mri', mname))\n",
    "    print(np.unique(mri_seg_orig))\n",
    "    mri_man_segs_orig.append(mri_seg_orig)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f3f6e83-4d5b-4dfa-81b9-bb7893184254",
   "metadata": {},
   "outputs": [],
   "source": [
    "adir = '/autofs/cluster/freesurfer/subjects/test/buckner_data/samseg'\n",
    "mname = 'aseg.mgz'\n",
    "vname = 'norm.mgz'\n",
    "sfile = os.path.join('/autofs/cluster/freesurfer/subjects/test/buckner_data/subjects.1-33.txt')\n",
    "\n",
    "with open(sfile, 'r') as f:\n",
    "    man_subjects = f.read().split('\\n')[0:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb8cd13-5865-489e-9ed5-6d0b670b1f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/33 [00:01<00:37,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/33 [00:02<00:35,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3/33 [00:03<00:34,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 4/33 [00:04<00:33,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 5/33 [00:05<00:33,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 6/33 [00:07<00:31,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 7/33 [00:08<00:30,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 8/33 [00:09<00:28,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 9/33 [00:10<00:28,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 10/33 [00:11<00:27,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 11/33 [00:12<00:25,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 12/33 [00:14<00:24,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   2   3   4   5   7   8  10  11  12  13  14  15  16  17  18  24  26\n",
      "  28  30  31  41  42  43  44  46  47  49  50  51  52  53  54  58  60  62\n",
      "  63  72  77  80  85 251 252 253 254 255]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 12/33 [00:15<00:26,  1.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m tqdm(man_subjects):\n\u001b[1;32m      4\u001b[0m     mri_seg_orig \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mload_volume(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(adir, s, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmri\u001b[39m\u001b[38;5;124m'\u001b[39m, mname))\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmri_seg_orig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m     mri_man_segs_orig\u001b[38;5;241m.\u001b[39mappend(mri_seg_orig)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mri_man_segs_orig = []\n",
    "\n",
    "for s in tqdm(man_subjects):\n",
    "    mri_seg_orig = sf.load_volume(os.path.join(adir, s, 'mri', mname))\n",
    "    print(np.unique(mri_seg_orig))\n",
    "    mri_man_segs_orig.append(mri_seg_orig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9625a4-13b3-4f0c-8676-b6813ea45d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: freesurfer package: set FS_SURFA_PORT env var to print surfa porting suggestions\n"
     ]
    }
   ],
   "source": [
    "import freesurfer as fs\n",
    "mri_segs_orig = [sf.load_volume(str(seg_files[i])) for i in tqdm(crop_indices)]\n",
    "mri_segs = [mri.fit_to_shape(target_shape, center='bbox') for mri in tqdm(mri_segs_orig)]\n",
    "mapping = fs.lookups.nonlateral_aseg_recoder()\n",
    "target_lut = mapping.target_lut\n",
    "lut_name = 'nonlat.txt'\n",
    "mri_segs_recoded = [fs.label.recode(mri, mapping) for mri in mri_segs]  \n",
    "lesion_label = target_lut.search('Left-Lesion')[0]\n",
    "np_segs = [vol.data for vol in mri_segs_recoded]\n",
    "labels_in = np.unique(np.array(np_segs)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159145ef-b62c-4a01-8e91-885e791b7337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d88e8b-aeed-4a02-a017-7ac41bb925ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

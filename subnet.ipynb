{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89244cdd-f4e6-47f0-aee7-bebe40010e7d",
   "metadata": {},
   "source": [
    "# Sub Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160121c4-d431-49f3-9649-3214873db010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 14:49:08.530454: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-06 14:49:08.576012: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "UserWarning: freesurfer package: set FS_SURFA_PORT env var to print surfa porting suggestions\n",
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Issue loading cv2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host name rtx-08.nmr.mgh.harvard.edu\n",
      "visible devices: 0,1\n",
      "using 3 gpus\n",
      "model_device /gpu:0, synth_device /gpu:1, dev_str 0, 1\n",
      "physical GPU # is None\n"
     ]
    }
   ],
   "source": [
    "import socket, os\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers as KL\n",
    "from tqdm import tqdm\n",
    "import glob, copy\n",
    "import scipy\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import sys\n",
    "# from freesurfer import deeplearn as fsd\n",
    "import freesurfer as fs\n",
    "from freesurfer.lookups import nonlateral_aseg_recoder\n",
    "\n",
    "import neurite as ne\n",
    "import neurite_sandbox as nes\n",
    "import voxelmorph as vxm\n",
    "import voxelmorph_sandbox as vxms\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import surfa as sf\n",
    "from utils import *\n",
    "\n",
    "import layer_dict as ld\n",
    "import pdb as gdb\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "log_dir = \"logs\"\n",
    "models_dir = \"models\"\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    \n",
    "weights_saver = PeriodicWeightsSaver(filepath=models_dir, save_freq=10)  # Save weights every 100 epochs\n",
    "\n",
    "TB_callback = CustomTensorBoard(\n",
    "    base_log_dir=log_dir,\n",
    "    histogram_freq=100,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "            \n",
    "vscale = 2\n",
    "vscale = 1\n",
    "\n",
    "dofit = True\n",
    "\n",
    "which_loss = 'both'\n",
    "which_loss = 'mse'\n",
    "which_loss = 'cce'\n",
    "which_loss = 'dice'\n",
    "\n",
    "same_contrast=False\n",
    "same_contrast=True\n",
    "\n",
    "oshapes = False\n",
    "oshapes = True\n",
    "\n",
    "fit_lin = True\n",
    "fit_lin = False\n",
    "\n",
    "save_model = False\n",
    "save_model = True\n",
    "\n",
    "doaff = False\n",
    "doaff = True\n",
    "\n",
    "test_adni = False\n",
    "\n",
    "\n",
    "model_dir = 'models'\n",
    "gpuid = -1\n",
    "host = socket.gethostname()\n",
    "from neurite_sandbox.tf.utils.utils import plot_fit_callback as pfc\n",
    "\n",
    "\n",
    "print(f'host name {socket.gethostname()}')\n",
    "print(\"visible devices:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "# ngpus = 1 if os.getenv('NGPUS') is None else int(os.getenv('NGPUS'))\n",
    "ngpus =len(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "print(f'using {ngpus} gpus')\n",
    "if ngpus > 1:\n",
    "    model_device = '/gpu:0'\n",
    "    synth_device = '/gpu:1'\n",
    "    synth_gpu = 1\n",
    "    dev_str = \"0, 1\"\n",
    "else:\n",
    "    model_device = '/gpu:0'\n",
    "    synth_device = model_device\n",
    "    synth_gpu = 0\n",
    "    dev_str = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = dev_str\n",
    "print(f'model_device {model_device}, synth_device {synth_device}, dev_str {dev_str}')\n",
    "print(f'physical GPU # is {os.getenv(\"SLURM_STEP_GPUS\")}')\n",
    "ret = ne.utils.setup_device(dev_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77edd9ca-e528-441b-9cbd-746501fbaa5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pytest-shutil in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from pytest-shutil) (1.14.0)\n",
      "Requirement already satisfied: execnet in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest-shutil) (2.0.2)\n",
      "Requirement already satisfied: contextlib2 in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest-shutil) (21.6.0)\n",
      "Requirement already satisfied: pytest in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest-shutil) (8.0.0)\n",
      "Requirement already satisfied: path.py in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest-shutil) (12.5.0)\n",
      "Requirement already satisfied: mock in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest-shutil) (5.1.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from pytest-shutil) (2.3.0)\n",
      "Requirement already satisfied: path in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from path.py->pytest-shutil) (16.10.0)\n",
      "Requirement already satisfied: iniconfig in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest->pytest-shutil) (2.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytest->pytest-shutil) (23.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.3.0 in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest->pytest-shutil) (1.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest->pytest-shutil) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /autofs/space/bal_004/users/jd1677/singularity-images/tensorflow_2.13.0-gpu/lib/python3.8/site-packages (from pytest->pytest-shutil) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# print(model_device,synth_device,ngpus,os.getenv('NGPUS'),os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "!pip install pytest-shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e0168d-fe61-4872-93b8-9890ee4e6c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dofit True, doaff True, fit_lin False, oshapes True, save_model True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 67893.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 10\n",
      "TRAINING model with loss dice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropping to (192, 192, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]UserWarning: fit_to_shape center argument no longer has any effect\n",
      "100%|██████████| 8/8 [00:00<00:00, 37.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding unique labels in 8 datasets...\n",
      "labels orig: [   0    2    4    5    7    8   10   11   12   13   14   15   16   17\n",
      "   18   24   26   28   30   31   41   43   44   46   47   49   50   51\n",
      "   52   53   54   58   60   62   63   72   77   80   85  251  252  253\n",
      "  254  255 1000 1001 1002 1003 1005 1006 1007 1008 1009 1010 1011 1012\n",
      " 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026\n",
      " 1027 1028 1029 1030 1031 1032 1033 1034 1035 2000 2001 2002 2003 2005\n",
      " 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019\n",
      " 2020 2021 2022 2023 2024 2025 2026 2027 2028 2029 2030 2031 2032 2033\n",
      " 2034 2035]\n",
      "target_lut:  0  Unknown                       (0   0   0   255)\n",
      "1  Left-Cerebral-White-Matter    (245 245 245 255)\n",
      "2  Left-Cerebral-Cortex          (205 62  78  255)\n",
      "3  CSF                           (120 18  134 255)\n",
      "4  Left-Cerebellum-White-Matter  (220 248 164 255)\n",
      "5  Left-Cerebellum-Cortex        (230 148 34  255)\n",
      "6  Left-Thalamus                 (0   118 14  255)\n",
      "7  Left-Caudate                  (122 186 220 255)\n",
      "8  Left-Putamen                  (236 13  176 255)\n",
      "9  Left-Pallidum                 (12  48  255 255)\n",
      "10 Brain-Stem                    (119 159 176 255)\n",
      "11 Left-Hippocampus              (220 216 20  255)\n",
      "12 Left-Amygdala                 (103 255 255 255)\n",
      "13 Left-Lesion                   (255 165 0   255)\n",
      "14 Left-Accumbens-area           (255 165 0   255)\n",
      "15 Left-VentralDC                (165 42  42  255)\n",
      "16 Left-Choroid-Plexus           (0   200 200 255)\n",
      "mri_segs_recoded:  (192, 192, 192) 16\n",
      "lesion_label: 13\n",
      "vol: (192, 192, 192)\n",
      "labels in:  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/990910_vc1265/mri/seg_edited.mgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:01,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/981102_vc604/mri/seg_edited.mgz\n",
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/981112_vc623/mri/seg_edited.mgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:00<00:01,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/981113_vc626/mri/seg_edited.mgz\n",
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/981204_vc660/mri/seg_edited.mgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:01<00:01,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/981216_vc681/mri/seg_edited.mgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:01<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/990104_vc700/mri/seg_edited.mgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:01<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/990111_vc716/mri/seg_edited.mgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:01<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/990114_vc722/mri/seg_edited.mgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:01<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mname: /autofs/cluster/freesurfer/subjects/atlases/aseg_atlas/990114_vc723/mri/seg_edited.mgz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.58it/s]\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mri_seg_atlas: (192, 192, 192, 17)\n",
      "(192, 192, 192)\n",
      "mri_norm_atlas: (192, 192, 192)\n",
      "norm atlas shape:  (1, 192, 192, 192, 1) (192, 192, 192) (192, 192, 192, 17)\n"
     ]
    }
   ],
   "source": [
    "import linecache\n",
    "import os, psutil\n",
    "from itertools import islice\n",
    "\n",
    "ntest = 5\n",
    "num_subjects=10\n",
    "# policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "\n",
    "# policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "# tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "# policy = tf.keras.mixed_precision.Policy('float32')\n",
    "# tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "print(f'dofit {dofit}, doaff {doaff}, fit_lin {fit_lin}, oshapes {oshapes}, save_model {save_model}')\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "adir = '/autofs/cluster/freesurfer/subjects/atlases/aseg_atlas'\n",
    "mname = 'seg_edited.mgz'\n",
    "vname = 'norm.mgz'\n",
    "sfile = os.path.join(adir, 'scripts', 'subjects.txt')\n",
    "\n",
    "# man_subjects = [linecache.getline(sfile, i).strip() for i in range(1, num_subjects + 1)]\n",
    "man_subjects = [line.strip() for line in islice(open(sfile), num_subjects)]\n",
    "\n",
    "\n",
    "crop = -1 if dofit else ntest\n",
    "\n",
    "# odir = '/autofs/vast/braindev/braindev/OASIS/OASIS1/synth-high-res/recon_subject'\n",
    "# subjects = [f for f in Path(odir).iterdir() if 'OASIS_OAS1_0' in str(f)]\n",
    "# seg_files = [f/f'mri/aparc+aseg.mgz' for f in tqdm(subjects)]\n",
    "# seg_files = [f/f'mri/aseg.mgz' for f in tqdm(subjects)]\n",
    "odir = '/autofs/vast/braindev/braindev/OASIS/OASIS1/synth-high-res/recon_subject'\n",
    "# subjects = [f for f in Path(odir).iterdir() if 'OASIS_OAS1_0' in str(f)]\n",
    "subjects = [f for f in islice(Path(odir).iterdir(), num_subjects) if 'OASIS_OAS1_0' in str(f)]\n",
    "\n",
    "seg_files = [f / 'mri/aparc+aseg.mgz' for f in tqdm(subjects[0:crop])]\n",
    "\n",
    "print(len(seg_files),len(subjects))\n",
    "if dofit:\n",
    "    print(f'TRAINING model with loss {which_loss}')\n",
    "else:\n",
    "    print(f'loading model trained with {which_loss} loss')\n",
    "\n",
    "target_shape = (192,)*3\n",
    "inshape = target_shape\n",
    "\n",
    "def minmax_norm(mri, axis=None):\n",
    "    \"\"\"\n",
    "    Min-max normalize an mri struct using a safe division.\n",
    "\n",
    "    Arguments:\n",
    "        x: np.array to be normalized\n",
    "        axis: Dimensions to reduce during normalization. If None, all axes will be considered,\n",
    "            treating the input as a single image. To normalize batches or features independently,\n",
    "            exclude the respective dimensions.\n",
    "\n",
    "    Returns:\n",
    "        Normalized tensor.\n",
    "    \"\"\"\n",
    "    x = mri.data\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    mri.data = (x - x_min) / (x_max - x_min) \n",
    "    return mri\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lut = fs.lookups.default()\n",
    "# lut = fs.lookups.LookupTable.read(os.path.join(odir, 'seg35_labels.txt'))\n",
    "# lut = fs.lookups.LookupTable.read(os.path.join(odir, f'seg{nb_labels}_labels.txt'))\n",
    "\n",
    "# csf = lut.search('CSF')[0]\n",
    "lesion_label_orig = lut.search('Left-Lesion')\n",
    "if len(lesion_label_orig) > 0:\n",
    "    lesion_label_orig = lesion_label_orig[0]\n",
    "else:   # not in the lut - add a new one\n",
    "    lesion_label_orig = 77\n",
    "    lut.add(lesion_label_orig, 'Lesion', color=[240,240,240])\n",
    "\n",
    "if 'inited' not in locals() and 'inited' not in globals():\n",
    "    inited = False\n",
    "\n",
    "# print(\"lut:\",lut)\n",
    "\n",
    "if not inited:\n",
    "    # mri_segs_orig = [fs.Volume.read(str(fname)) for fname in tqdm(seg_files[:crop])]\n",
    "    mri_segs_orig = [sf.load_volume(str(fname)) for fname in tqdm(seg_files[:crop])]\n",
    "\n",
    "    # mri_segs = [sf.load_volume(str(fname)) for fname in tqdm(seg_files[:crop])]\n",
    "    if vscale > 1:\n",
    "        print(f'downsampling by {vscale}')\n",
    "        mri_segs = [mri.reslice(vscale) for mri in tqdm(mri_segs_orig)]\n",
    "    else:\n",
    "        print(f'cropping to {target_shape}')\n",
    "        mri_segs = [mri.fit_to_shape(target_shape, center='bbox') for mri in tqdm(mri_segs_orig)]\n",
    "        # mri_segs = [mri.reshape(target_shape, center='bbox') for mri in tqdm(mri_segs)]\n",
    "\n",
    "    np_segs_orig = [mri.data for mri in mri_segs]\n",
    "    print(f'finding unique labels in {len(mri_segs_orig)} datasets...')\n",
    "    labels_orig = np.unique(np.array(np_segs_orig))\n",
    "    print(\"labels orig:\",labels_orig)\n",
    "    # mapping = fs.lookups.tissue_type_recoder_no_skull(include_lesions=use_lesions)\n",
    "    mapping = fs.lookups.nonlateral_aseg_recoder()\n",
    "    target_lut = mapping.target_lut\n",
    "    print(\"target_lut: \",target_lut)\n",
    "    lut_name = 'nonlat.txt'\n",
    "    mri_segs_recoded = [fs.label.recode(mri, mapping) for mri in mri_segs]  \n",
    "    print(\"mri_segs_recoded: \", mri_segs_recoded[0].shape,np.max(mri_segs_recoded[0]))\n",
    "    lesion_label = target_lut.search('Left-Lesion')[0]\n",
    "    print(\"lesion_label:\",lesion_label)\n",
    "    np_segs = [vol.data for vol in mri_segs_recoded]\n",
    "    print(\"vol:\", mri_segs_recoded[0].shape)\n",
    "    labels_in = np.unique(np.array(np_segs)).astype(int)\n",
    "    print(\"labels in: \", labels_in)\n",
    "    if lesion_label not in labels_in:\n",
    "        l = list(labels_in)\n",
    "        l.append(lesion_label)\n",
    "        labels_in = np.array(l)\n",
    "    nlabels_small = len(labels_in)\n",
    "    label_map = {}\n",
    "    keys = mapping.mapping.keys()\n",
    "    # print(\"keys:\",keys)\n",
    "    lab_to_ind = np.zeros((labels_orig.max()+1,), dtype=np.uint8)\n",
    "    for label in labels_orig:\n",
    "        if label not in keys:\n",
    "            output_label = 0\n",
    "        else:\n",
    "            output_label = mapping.mapping[label]\n",
    "        label_map[label] = output_label\n",
    "        lab_to_ind[label] = output_label\n",
    "\n",
    "\n",
    "    mri_man_segs = []  # manual segs\n",
    "    mri_norms = []  # mri vols\n",
    "    mri_norms_orig = []\n",
    "    mri_man_segs_orig = []\n",
    "\n",
    "    for s in tqdm(man_subjects):\n",
    "        # mri_seg_orig = fs.Volume.read(os.path.join(adir, s, 'mri', mname))\n",
    "        print(\"mname:\",os.path.join(adir, s, 'mri', mname))\n",
    "        mri_seg_orig = sf.load_volume(os.path.join(adir, s, 'mri', mname))\n",
    "\n",
    "    \n",
    "        mri_man_segs_orig.append(mri_seg_orig)\n",
    "        # mri_seg = mri_seg_orig.fit_to_shape(target_shape, center='bbox')\n",
    "        mri_seg = mri_seg_orig.reshape(target_shape)\n",
    "        mri_man_segs.append(mri_seg)\n",
    "        # mri_norm_orig = fs.Volume.read(os.path.join(adir, s, 'mri', vname))\n",
    "        mri_norm_orig = sf.load_volume(os.path.join(adir, s, 'mri', vname))\n",
    "\n",
    "        mri_norm = mri_norm_orig.resample_like(mri_seg)\n",
    "        mri_norms.append(mri_norm)\n",
    "        mri_norms_orig.append(mri_norm_orig)\n",
    "\n",
    "    mri_man_segs_recoded = [fs.label.recode(mri, mapping) for mri in tqdm(mri_man_segs)]\n",
    "\n",
    "    # mri_seg_atlas = fs.Volume.read(\"aseg_atlas.mgz\")\n",
    "    mri_seg_atlas = sf.load_volume(\"aseg_atlas.mgz\").reshape(target_shape)\n",
    "    print(\"mri_seg_atlas:\",mri_seg_atlas.shape)\n",
    "    hard_seg = np.argmax(mri_seg_atlas.data, axis=-1)\n",
    "    mri_hard_seg = mri_seg_atlas.copy()\n",
    "    mri_hard_seg.data = hard_seg\n",
    "    mri_hard_seg_cropped = mri_hard_seg.reshape(target_shape)\n",
    "    print(mri_hard_seg_cropped.shape)\n",
    "    mri_norm_atlas = sf.load_volume(\"norm_atlas.mgz\").resample_like(mri_hard_seg)\n",
    "    print(\"mri_norm_atlas:\",mri_norm_atlas.shape)\n",
    "    # mri_hard_seg = mri_seg_atlas.copy(hard_seg).fit_to_shape(target_shape, center='bbox')\n",
    "    # mri_norm_atlas = fs.Volume.read(\"norm_atlas.mgz\").resample_like(mri_hard_seg)\n",
    "\n",
    "    mri_seg_atlas = mri_seg_atlas.resample_like(mri_hard_seg)\n",
    "    norm_atlas = (mri_norm_atlas.data / mri_norm_atlas.data.max())[np.newaxis, ..., np.newaxis]\n",
    "    print(\"norm atlas shape: \", norm_atlas.shape,mri_hard_seg.shape,mri_seg_atlas.shape)\n",
    "    inited = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6011aaa6-877f-4edb-b899-3afecd000381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2590041\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(mri_man_segs_recoded[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e75cda-79cb-4e27-b10b-e70fc0ab0f7e",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c186f931-4ed3-441a-b126-e938f7cc7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using warp max = 2 and nlabels 17, fscale 1.1\n",
      "len unet_nf 6 [[42, 42], [46, 46], [50, 50], [55, 55], [61, 61], [67, 67]]\n"
     ]
    }
   ],
   "source": [
    "warp_max=2.5\n",
    "warp_max=2.1\n",
    "warp_max=2   \n",
    "warp_min=.5\n",
    "warp_blur_min=np.array([2, 4, 8])\n",
    "warp_blur_max=warp_blur_min*2\n",
    "bias_blur_min=np.array([2, 4, 8])\n",
    "bias_blur_max=bias_blur_min*2\n",
    "\n",
    "fscale = 1\n",
    "fscale = 1.75\n",
    "fscale = 1.5   # matches P32 N 16\n",
    "fscale = 1.1\n",
    "nfeats = 64\n",
    "nfeats = 42\n",
    "nsmall = 0\n",
    "nb_levels = int(np.log2(inshape[0]))-(1+nsmall)   # 4,4,4 is lowest level\n",
    "nb_conv_per_level = 2\n",
    "unet_scale = 1\n",
    "nfeats_small = int(nfeats // 3)\n",
    "unet_nf = []\n",
    "\n",
    "\n",
    "print(f'using warp max = {warp_max} and nlabels {nlabels_small}, fscale {fscale}')\n",
    "inshape=np_segs[0].shape\n",
    "gen_args = dict(\n",
    "    warp_min=warp_min,\n",
    "    warp_max=warp_max,\n",
    "    blur_max=2,  # was .5, then 1\n",
    "    bias_max=.25,  # was 2 then .5\n",
    "    bias_blur_min=bias_blur_min,\n",
    "    bias_blur_max=bias_blur_max,\n",
    "    gamma=0,\n",
    "    # warp_zero_mean=True,\n",
    "    zero_background=.75,\n",
    "    noise_max=.2,   \n",
    "    noise_min=.1\n",
    ")\n",
    "\n",
    "def synth_gen(label_vols, gen_model, lab_to_ind, labels_in, batch_size=8, use_rand=True, gpuid=1, \n",
    "              seg_resize=1, num_outside_shapes_to_add=8, use_log=False, debug=False, \n",
    "              add_outside=True):\n",
    "\n",
    "    inshape = label_vols[0].shape\n",
    "    nlabels = gen_model.outputs[-1].get_shape().as_list()[-1]  # number of compressed labels\n",
    "\n",
    "    if add_outside:\n",
    "        l2l = nes.models.labels_to_labels(\n",
    "            labels_in,\n",
    "            shapes_num=num_outside_shapes_to_add,\n",
    "            in_shape=label_vols[0].shape,\n",
    "            shapes_add=True\n",
    "        )\n",
    "        li = np.concatenate([labels_in, np.arange(labels_in.max()+1, \n",
    "                                                  labels_in.max()+1+num_outside_shapes_to_add)])\n",
    "        l2i = nes.models.labels_to_image(\n",
    "            labels_in=li,\n",
    "            labels_out=None,\n",
    "            in_shape=label_vols[0].shape,\n",
    "            zero_background=.5,  #  was 1\n",
    "            noise_max=.2,\n",
    "            noise_min=.1,\n",
    "            warp_max=0\n",
    "        )\n",
    "\n",
    "    # outputs [6] and [7] are the t2 labels without (6) and with (7) atrophy\n",
    "    batch_input_labels = np.zeros((batch_size, *inshape, 1))\n",
    "    label_shape = tuple(np.array(inshape) // seg_resize)\n",
    "    batch_onehots = np.zeros((batch_size, *label_shape, nlabels))\n",
    "    batch_images = np.zeros((batch_size, *inshape, 1))\n",
    "\n",
    "    if debug:\n",
    "        batch_orig_images1 = np.zeros((batch_size, *inshape, 1))\n",
    "        batch_orig_images2 = np.zeros((batch_size, *inshape, 1))\n",
    "        batch_orig_labels1 = np.zeros((batch_size, *label_shape, 1))\n",
    "        batch_orig_labels2 = np.zeros((batch_size, *label_shape, 1))\n",
    "\n",
    "    if gpuid >= 0:\n",
    "        device = '/gpu:' + str(gpuid)\n",
    "    else:\n",
    "        device = '/physical_device:CPU:0'\n",
    "        device = '/cpu:0'\n",
    "    print(device)\n",
    "    ind = -1\n",
    "    while (True):\n",
    "        for bind in range(batch_size):\n",
    "            if use_rand:\n",
    "                ind = np.random.randint(0, len(label_vols))\n",
    "            else:\n",
    "                ind = np.mod(ind+1, len(label_vols))\n",
    "            batch_input_labels[bind,...] = label_vols[ind].data[...,np.newaxis]\n",
    "\n",
    "        with tf.device(device):\n",
    "            pred = gen_model.predict_on_batch(batch_input_labels)\n",
    "\n",
    "        for bind in range(batch_size):\n",
    "            im = pred[0][bind,...]\n",
    "            onehot = pred[1][bind,...]\n",
    "            # if add_outside:\n",
    "                # im = nes.utils.augment.add_outside_shapes(im[..., 0], np.argmax(onehot, axis=-1), labels_in, l2l=l2l, l2i=l2i)[..., np.newaxis]\n",
    "\n",
    "            if use_log:\n",
    "                onehot[onehot == 0] = -10\n",
    "                onehot[onehot == 1] = 10\n",
    "\n",
    "            batch_images[bind, ...] = im\n",
    "            batch_onehots[bind, ...] = onehot\n",
    "\n",
    "        inputs = [batch_images]\n",
    "        outputs = [batch_onehots]\n",
    "\n",
    "        yield inputs, outputs\n",
    "                \n",
    "    return 0\n",
    "\n",
    "\n",
    "\n",
    "for level in range(nb_levels-nsmall):\n",
    "    filters_in_this_level = []\n",
    "    for layer in range(nb_conv_per_level):\n",
    "        filters_in_this_level.append(int(fscale**level*nfeats))\n",
    "        \n",
    "    unet_nf.append(filters_in_this_level)\n",
    "\n",
    "print(\"len unet_nf\",len(unet_nf), unet_nf)\n",
    "unet_nf = [[nfeats_small] * nb_conv_per_level] * nsmall + unet_nf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b12edbeb-4eef-437e-88b6-823c660e49cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-06 14:50:16.379287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46871 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:3e:00.0, compute capability: 8.6\n",
      "2024-02-06 14:50:16.380159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 47377 MB memory:  -> device: 1, name: Quadro RTX 8000, pci bus id: 0000:3f:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet model created with nf [[42, 42], [46, 46], [50, 50], [55, 55], [61, 61], [67, 67]]\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " unet_input (InputLayer)     [(None, 192, 192, 192, 1)]   0         []                            \n",
      "                                                                                                  \n",
      " unet_conv_downarm_0_0 (Con  (None, 192, 192, 192, 42)    1176      ['unet_input[0][0]']          \n",
      " v3D)                                                                                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_0_1 (Con  (None, 192, 192, 192, 42)    47670     ['unet_conv_downarm_0_0[0][0]'\n",
      " v3D)                                                               ]                             \n",
      "                                                                                                  \n",
      " unet_maxpool_0 (MaxPooling  (None, 96, 96, 96, 42)       0         ['unet_conv_downarm_0_1[0][0]'\n",
      " 3D)                                                                ]                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_1_0 (Con  (None, 96, 96, 96, 46)       52210     ['unet_maxpool_0[0][0]']      \n",
      " v3D)                                                                                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_1_1 (Con  (None, 96, 96, 96, 46)       57178     ['unet_conv_downarm_1_0[0][0]'\n",
      " v3D)                                                               ]                             \n",
      "                                                                                                  \n",
      " unet_maxpool_1 (MaxPooling  (None, 48, 48, 48, 46)       0         ['unet_conv_downarm_1_1[0][0]'\n",
      " 3D)                                                                ]                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_2_0 (Con  (None, 48, 48, 48, 50)       62150     ['unet_maxpool_1[0][0]']      \n",
      " v3D)                                                                                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_2_1 (Con  (None, 48, 48, 48, 50)       67550     ['unet_conv_downarm_2_0[0][0]'\n",
      " v3D)                                                               ]                             \n",
      "                                                                                                  \n",
      " unet_maxpool_2 (MaxPooling  (None, 24, 24, 24, 50)       0         ['unet_conv_downarm_2_1[0][0]'\n",
      " 3D)                                                                ]                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_3_0 (Con  (None, 24, 24, 24, 55)       74305     ['unet_maxpool_2[0][0]']      \n",
      " v3D)                                                                                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_3_1 (Con  (None, 24, 24, 24, 55)       81730     ['unet_conv_downarm_3_0[0][0]'\n",
      " v3D)                                                               ]                             \n",
      "                                                                                                  \n",
      " unet_maxpool_3 (MaxPooling  (None, 12, 12, 12, 55)       0         ['unet_conv_downarm_3_1[0][0]'\n",
      " 3D)                                                                ]                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_4_0 (Con  (None, 12, 12, 12, 61)       90646     ['unet_maxpool_3[0][0]']      \n",
      " v3D)                                                                                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_4_1 (Con  (None, 12, 12, 12, 61)       100528    ['unet_conv_downarm_4_0[0][0]'\n",
      " v3D)                                                               ]                             \n",
      "                                                                                                  \n",
      " unet_maxpool_4 (MaxPooling  (None, 6, 6, 6, 61)          0         ['unet_conv_downarm_4_1[0][0]'\n",
      " 3D)                                                                ]                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_5_0 (Con  (None, 6, 6, 6, 67)          110416    ['unet_maxpool_4[0][0]']      \n",
      " v3D)                                                                                             \n",
      "                                                                                                  \n",
      " unet_conv_downarm_5_1 (Con  (None, 6, 6, 6, 67)          121270    ['unet_conv_downarm_5_0[0][0]'\n",
      " v3D)                                                               ]                             \n",
      "                                                                                                  \n",
      " unet_up_6 (UpSampling3D)    (None, 12, 12, 12, 67)       0         ['unet_conv_downarm_5_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " unet_merge_6 (Concatenate)  (None, 12, 12, 12, 128)      0         ['unet_conv_downarm_4_1[0][0]'\n",
      "                                                                    , 'unet_up_6[0][0]']          \n",
      "                                                                                                  \n",
      " unet_conv_uparm_6_0 (Conv3  (None, 12, 12, 12, 61)       210877    ['unet_merge_6[0][0]']        \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " unet_conv_uparm_6_1 (Conv3  (None, 12, 12, 12, 61)       100528    ['unet_conv_uparm_6_0[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " unet_up_7 (UpSampling3D)    (None, 24, 24, 24, 61)       0         ['unet_conv_uparm_6_1[0][0]'] \n",
      "                                                                                                  \n",
      " unet_merge_7 (Concatenate)  (None, 24, 24, 24, 116)      0         ['unet_conv_downarm_3_1[0][0]'\n",
      "                                                                    , 'unet_up_7[0][0]']          \n",
      "                                                                                                  \n",
      " unet_conv_uparm_7_0 (Conv3  (None, 24, 24, 24, 55)       172315    ['unet_merge_7[0][0]']        \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " unet_conv_uparm_7_1 (Conv3  (None, 24, 24, 24, 55)       81730     ['unet_conv_uparm_7_0[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " unet_up_8 (UpSampling3D)    (None, 48, 48, 48, 55)       0         ['unet_conv_uparm_7_1[0][0]'] \n",
      "                                                                                                  \n",
      " unet_merge_8 (Concatenate)  (None, 48, 48, 48, 105)      0         ['unet_conv_downarm_2_1[0][0]'\n",
      "                                                                    , 'unet_up_8[0][0]']          \n",
      "                                                                                                  \n",
      " unet_conv_uparm_8_0 (Conv3  (None, 48, 48, 48, 50)       141800    ['unet_merge_8[0][0]']        \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " unet_conv_uparm_8_1 (Conv3  (None, 48, 48, 48, 50)       67550     ['unet_conv_uparm_8_0[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " unet_up_9 (UpSampling3D)    (None, 96, 96, 96, 50)       0         ['unet_conv_uparm_8_1[0][0]'] \n",
      "                                                                                                  \n",
      " unet_merge_9 (Concatenate)  (None, 96, 96, 96, 96)       0         ['unet_conv_downarm_1_1[0][0]'\n",
      "                                                                    , 'unet_up_9[0][0]']          \n",
      "                                                                                                  \n",
      " unet_conv_uparm_9_0 (Conv3  (None, 96, 96, 96, 46)       119278    ['unet_merge_9[0][0]']        \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " unet_conv_uparm_9_1 (Conv3  (None, 96, 96, 96, 46)       57178     ['unet_conv_uparm_9_0[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " unet_up_10 (UpSampling3D)   (None, 192, 192, 192, 46)    0         ['unet_conv_uparm_9_1[0][0]'] \n",
      "                                                                                                  \n",
      " unet_merge_10 (Concatenate  (None, 192, 192, 192, 88)    0         ['unet_conv_downarm_0_1[0][0]'\n",
      " )                                                                  , 'unet_up_10[0][0]']         \n",
      "                                                                                                  \n",
      " unet_conv_uparm_10_0 (Conv  (None, 192, 192, 192, 42)    99834     ['unet_merge_10[0][0]']       \n",
      " 3D)                                                                                              \n",
      "                                                                                                  \n",
      " unet_conv_uparm_10_1 (Conv  (None, 192, 192, 192, 42)    47670     ['unet_conv_uparm_10_0[0][0]']\n",
      " 3D)                                                                                              \n",
      "                                                                                                  \n",
      " unet_likelihood (Conv3D)    (None, 192, 192, 192, 17)    731       ['unet_conv_uparm_10_1[0][0]']\n",
      "                                                                                                  \n",
      " unet_prediction (Activatio  (None, 192, 192, 192, 17)    0         ['unet_likelihood[0][0]']     \n",
      " n)                                                                                               \n",
      "                                                                                                  \n",
      " seg (Softmax)               (None, 192, 192, 192, 17)    0         ['unet_prediction[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1966320 (7.50 MB)\n",
      "Trainable params: 1966320 (7.50 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/deprecation.py:648: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "using warp max = 2 and nlabels 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: nes.utils.generative.random_blur_rescale is deprecated and will be removed in the near future. Please use ne.utils.augment.random_blur_rescale instead.\n",
      "UserWarning: In the future, the default value of argument `clip_max` to nes.models.labels_to_image will change to None.\n",
      "UserWarning: In the future, the default value of argument `slice_stride_min` to nes.models.labels_to_image will change to 1.\n",
      "UserWarning: In the future, the default value of argument `slice_stride_max` to nes.models.labels_to_image will change to 8.\n",
      "UserWarning: In the future, nes.models.labels_to_image will apply the bias field just before adding noise. Use the new code by passing `bias_first=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 192, 192, 192, 1)\n",
      "(None, 192, 192, 192, 1)\n"
     ]
    }
   ],
   "source": [
    "import generators as gens\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "lr = 1e-5\n",
    "lr = 1e-4\n",
    "lr_lin = 1e-4\n",
    "thresh = -.2*2 \n",
    "cooldown = 25\n",
    "patience = 600\n",
    "\n",
    "name = f'aseg.outside.unet_nf.{nfeats}.{nfeats_small}.{nsmall}.levels.{nb_levels}.warp_max.{warp_max}.oshapes.{oshapes}.fscale.{fscale}'\n",
    "\n",
    "label_weights = np.ones((1,nlabels_small,))\n",
    "# label_weights[0,-1] = .01  # downweight lesion class\n",
    "lfunc = ne.losses.Dice(nb_labels=nlabels_small, weights=None, check_input_limits=False).mean_loss\n",
    "\n",
    "\n",
    "mc_cb_lin = tf.keras.callbacks.ModelCheckpoint(name+'.checkpoint.lin.tf', save_best_only=False)\n",
    "write_cb_lin = nes.callbacks.WriteHist(name+'.lin.txt')\n",
    "\n",
    "unet_device = model_device if (fit_lin or dofit) else synth_device\n",
    "with tf.device(unet_device): \n",
    "    model_lin = ne.models.unet(unet_nf, inshape+(1,), None, 3, nlabels_small, feat_mult=None, final_pred_activation='linear')\n",
    "    print(f'unet model created with nf {unet_nf}')\n",
    "    softmax_out = KL.Softmax(name='seg')(model_lin.outputs[0])\n",
    "    model = tf.keras.Model(model_lin.inputs, [softmax_out])\n",
    "    model.summary()\n",
    "\n",
    "vxm_model = gens.read_vxm_model(inshape)\n",
    "vxm_smooth_wt = np.zeros((1, 1))\n",
    "vxm_smooth_wt[0,0] = .3   # warp regularization hyper parameter\n",
    "\n",
    "gen_model = gens.create_gen_model(np_segs, oshapes, synth_device, nlabels_small, labels_in, inshape, warp_max)\n",
    "\n",
    "if fit_lin and dofit:\n",
    "    thresh_lin = 5\n",
    "    thresh = -.5\n",
    "    print(f'thresh {thresh}, thresh_lin {thresh_lin}')\n",
    "    lr_cb_lin = nes.tf.callbacks.ReduceLRWithModelCheckpointAndRecovery(name+'.lin.h5', monitor='loss',\n",
    "                                                                        verbose=1, cooldown=cooldown, \n",
    "                                                                        recovery_decrease_factor=.8,\n",
    "                                                                        factor=.8, patience=patience, \n",
    "                                                                        thresh_increase_factor=2,\n",
    "                                                                        thresh=thresh_lin,\n",
    "                                                                        save_weights_only=True, \n",
    "                                                                        burn_in=10,\n",
    "                                                                        min_lr=1e-7,\n",
    "                                                                        warm_restart_epoch=None,\n",
    "                                                                        nloss=5,\n",
    "                                                                        warm_restart_lr=None)\n",
    "\n",
    "    callbacks_lin = [lr_cb_lin, write_cb_lin, ne.callbacks.LRLog(), mc_cb_lin]\n",
    "    callbacks_lin = [lr_cb_lin, write_cb_lin, ne.callbacks.LRLog()]\n",
    "    lfunc_lin = ne.losses.MeanSquaredErrorProb().loss\n",
    "    with tf.device(synth_device):\n",
    "        gen_lin = synth_gen(mri_segs_recoded, gen_model, None, labels_in, batch_size=batch_size, \n",
    "                            use_rand=True, gpuid=synth_gpu,use_log=True, add_outside=oshapes)\n",
    "        gen = synth_gen(mri_segs_recoded, gen_model, None, labels_in, batch_size=batch_size, \n",
    "                        use_rand=True, gpuid=synth_gpu, debug=False, add_outside=oshapes)\n",
    "        vgen = gens.real_gen(mri_man_segs_recoded, mri_norms, vxm_model, norm_atlas, \n",
    "                             None, labels_in, \n",
    "                             batch_size=batch_size, use_rand=True, \n",
    "                             gpuid=synth_gpu, debug=False, add_outside=oshapes)\n",
    "\n",
    "    loss_funcs_lin = [lfunc_lin]\n",
    "    print(f'fitting linear model and saving hist to {write_cb_lin.fname} and model to {lr_cb_lin.fname}')\n",
    "    with tf.device(model_device):\n",
    "        nes.utils.check_and_compile(model_lin, gen_lin, optimizer= Adam(learning_rate=lr_lin), \n",
    "                                    loss=loss_funcs_lin, check_layers=False, run_eagerly=True)\n",
    "        fhist_lin = model_lin.fit(gen_lin, epochs=int(50), steps_per_epoch=50, initial_epoch=0, callbacks=callbacks_lin)\n",
    "        model_lin.save_weights(lr_cb_lin.fname)\n",
    "else:\n",
    "    fname = name + '.lin.h5'\n",
    "    if os.path.exists(fname) and dofit:\n",
    "        print(f'loading linear weights from {fname}')\n",
    "        model_lin.load_weights(fname)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a46469-2e01-4f1d-b7b8-e6fd3c1c52a5",
   "metadata": {},
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20fb6c-91f5-4a3d-b6c6-86d098b6ade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr callback created with thresh -0.4\n",
      "checking and compiling model:\n",
      "batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: nes.utils.generative.random_blur_rescale is deprecated and will be removed in the near future. Please use ne.utils.augment.random_blur_rescale instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving fit results to aseg.outside.unet_nf.42.14.0.levels.6.warp_max.2.oshapes.True.fscale.1.1.h5 and hist to aseg.outside.unet_nf.42.14.0.levels.6.warp_max.2.oshapes.True.fscale.1.1.txt\n",
      "batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "Epoch 1/2\n",
      "batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 2/50 [>.............................] - ETA: 49s - loss: -0.1648 batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 3/50 [>.............................] - ETA: 1:30 - loss: -0.1308batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 4/50 [=>............................] - ETA: 2:03 - loss: -0.1125batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 5/50 [==>...........................] - ETA: 2:17 - loss: -0.1039batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 6/50 [==>...........................] - ETA: 2:24 - loss: -0.1022batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 7/50 [===>..........................] - ETA: 2:27 - loss: -0.1062batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 8/50 [===>..........................] - ETA: 2:28 - loss: -0.1100batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 9/50 [====>.........................] - ETA: 2:28 - loss: -0.1152batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "10/50 [=====>........................] - ETA: 2:27 - loss: -0.1226batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "11/50 [=====>........................] - ETA: 2:25 - loss: -0.1298batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "12/50 [======>.......................] - ETA: 2:23 - loss: -0.1368batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "13/50 [======>.......................] - ETA: 2:20 - loss: -0.1422batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "14/50 [=======>......................] - ETA: 2:18 - loss: -0.1444batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "15/50 [========>.....................] - ETA: 2:15 - loss: -0.1445batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "16/50 [========>.....................] - ETA: 2:11 - loss: -0.1467batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "17/50 [=========>....................] - ETA: 2:08 - loss: -0.1522batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "18/50 [=========>....................] - ETA: 2:05 - loss: -0.1566batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "19/50 [==========>...................] - ETA: 2:01 - loss: -0.1584batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "20/50 [===========>..................] - ETA: 1:58 - loss: -0.1609batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "21/50 [===========>..................] - ETA: 1:54 - loss: -0.1662batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "22/50 [============>.................] - ETA: 1:51 - loss: -0.1697batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "23/50 [============>.................] - ETA: 1:47 - loss: -0.1725batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "24/50 [=============>................] - ETA: 1:43 - loss: -0.1762batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "25/50 [==============>...............] - ETA: 1:40 - loss: -0.1801batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "26/50 [==============>...............] - ETA: 1:36 - loss: -0.1842batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "27/50 [===============>..............] - ETA: 1:32 - loss: -0.1872batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "28/50 [===============>..............] - ETA: 1:28 - loss: -0.1904batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "29/50 [================>.............] - ETA: 1:24 - loss: -0.1942batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "30/50 [=================>............] - ETA: 1:20 - loss: -0.1975batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "31/50 [=================>............] - ETA: 1:16 - loss: -0.2008batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "32/50 [==================>...........] - ETA: 1:12 - loss: -0.2045batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "33/50 [==================>...........] - ETA: 1:08 - loss: -0.2081batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "34/50 [===================>..........] - ETA: 1:04 - loss: -0.2117batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "35/50 [====================>.........] - ETA: 1:00 - loss: -0.2145batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "36/50 [====================>.........] - ETA: 56s - loss: -0.2179 batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "37/50 [=====================>........] - ETA: 52s - loss: -0.2214batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "38/50 [=====================>........] - ETA: 48s - loss: -0.2243batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "39/50 [======================>.......] - ETA: 44s - loss: -0.2274batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "40/50 [=======================>......] - ETA: 40s - loss: -0.2303batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "41/50 [=======================>......] - ETA: 36s - loss: -0.2335batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "42/50 [========================>.....] - ETA: 32s - loss: -0.2366batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "43/50 [========================>.....] - ETA: 28s - loss: -0.2393batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "44/50 [=========================>....] - ETA: 24s - loss: -0.2428batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "45/50 [==========================>...] - ETA: 20s - loss: -0.2460batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "46/50 [==========================>...] - ETA: 16s - loss: -0.2486batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "47/50 [===========================>..] - ETA: 12s - loss: -0.2510batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "48/50 [===========================>..] - ETA: 8s - loss: -0.2539 batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "49/50 [============================>.] - ETA: 4s - loss: -0.2567batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "1/1 [==============================] - 2s 2s/step- loss: -0.25\n",
      "batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "50/50 [==============================] - 244s 5s/step - loss: -0.2595 - val_loss: -0.4003\n",
      "Epoch 2/2\n",
      "batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 3/50 [>.............................] - ETA: 49s - loss: -0.3863batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 4/50 [=>............................] - ETA: 1:13 - loss: -0.3861batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 5/50 [==>...........................] - ETA: 1:40 - loss: -0.3899batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 6/50 [==>...........................] - ETA: 1:56 - loss: -0.3867batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 7/50 [===>..........................] - ETA: 2:04 - loss: -0.3917batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 8/50 [===>..........................] - ETA: 2:09 - loss: -0.3920batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      " 9/50 [====>.........................] - ETA: 2:12 - loss: -0.3954batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "10/50 [=====>........................] - ETA: 2:13 - loss: -0.3916batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "11/50 [=====>........................] - ETA: 2:13 - loss: -0.3962batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "12/50 [======>.......................] - ETA: 2:12 - loss: -0.4000batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "13/50 [======>.......................] - ETA: 2:20 - loss: -0.4000batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "14/50 [=======>......................] - ETA: 2:16 - loss: -0.4050batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "15/50 [========>.....................] - ETA: 2:13 - loss: -0.4065batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "16/50 [========>.....................] - ETA: 2:10 - loss: -0.4091batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "17/50 [=========>....................] - ETA: 2:07 - loss: -0.4112batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "18/50 [=========>....................] - ETA: 2:04 - loss: -0.4130batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "19/50 [==========>...................] - ETA: 2:01 - loss: -0.4149batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "20/50 [===========>..................] - ETA: 1:57 - loss: -0.4173batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "21/50 [===========>..................] - ETA: 1:54 - loss: -0.4203batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "22/50 [============>.................] - ETA: 1:50 - loss: -0.4214batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "23/50 [============>.................] - ETA: 1:46 - loss: -0.4233batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "24/50 [=============>................] - ETA: 1:43 - loss: -0.4254batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "25/50 [==============>...............] - ETA: 1:39 - loss: -0.4266batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "26/50 [==============>...............] - ETA: 1:35 - loss: -0.4287batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "27/50 [===============>..............] - ETA: 1:31 - loss: -0.4311batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "28/50 [===============>..............] - ETA: 1:28 - loss: -0.4331batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "29/50 [================>.............] - ETA: 1:24 - loss: -0.4353batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "30/50 [=================>............] - ETA: 1:20 - loss: -0.4374batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "31/50 [=================>............] - ETA: 1:16 - loss: -0.4384batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "32/50 [==================>...........] - ETA: 1:12 - loss: -0.4384batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "33/50 [==================>...........] - ETA: 1:08 - loss: -0.4399batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "34/50 [===================>..........] - ETA: 1:04 - loss: -0.4385batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "35/50 [====================>.........] - ETA: 1:00 - loss: -0.4377batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "36/50 [====================>.........] - ETA: 56s - loss: -0.4384 batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "37/50 [=====================>........] - ETA: 52s - loss: -0.4387batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "38/50 [=====================>........] - ETA: 48s - loss: -0.4402batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "39/50 [======================>.......] - ETA: 44s - loss: -0.4415batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "40/50 [=======================>......] - ETA: 40s - loss: -0.4416batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "41/50 [=======================>......] - ETA: 36s - loss: -0.4434batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "42/50 [========================>.....] - ETA: 32s - loss: -0.4448batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "43/50 [========================>.....] - ETA: 28s - loss: -0.4449batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "44/50 [=========================>....] - ETA: 24s - loss: -0.4466batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "45/50 [==========================>...] - ETA: 20s - loss: -0.4472batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "46/50 [==========================>...] - ETA: 16s - loss: -0.4474batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "47/50 [===========================>..] - ETA: 12s - loss: -0.4489batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "48/50 [===========================>..] - ETA: 8s - loss: -0.4504 batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "49/50 [============================>.] - ETA: 4s - loss: -0.4507batch inpu: (1, 192, 192, 192, 1)  batch norm atlas: (1, 192, 192, 192, 1)\n",
      "50/50 [==============================] - ETA: 0s - loss: -0.4521"
     ]
    }
   ],
   "source": [
    "patch_size = 16\n",
    "initial_epoch = 0\n",
    "ntest = 5\n",
    "num_epochs = 2\n",
    "with tf.device(synth_device):\n",
    "    val_size = 20\n",
    "    gen = gens.synth_gen(mri_segs_recoded, gen_model, vxm_model, norm_atlas, \n",
    "                         None, labels_in, batch_size=batch_size, \n",
    "                         use_rand=True, gpuid=synth_gpu, debug=False, add_outside=oshapes,\n",
    "                         zero_background=.1)\n",
    "    vgen = gens.real_gen(mri_man_segs_recoded, mri_norms, vxm_model, norm_atlas, \n",
    "                         None, labels_in, \n",
    "                         batch_size=batch_size, use_rand=True, \n",
    "                         gpuid=synth_gpu, debug=False, add_outside=oshapes)\n",
    "    if 0:\n",
    "        vgen = gens.synth_gen(mri_segs_recoded[len(mri_segs_recoded)-val_size:], gen_model, None, labels_in, \n",
    "                              batch_size=batch_size, use_rand=True, \n",
    "                              gpuid=synth_gpu, debug=False, add_outside=oshapes)\n",
    "\n",
    "\n",
    "\n",
    "write_cb = nes.callbacks.WriteHist(name+'.txt', mode='w' if initial_epoch == 0 else 'a')\n",
    "if initial_epoch > 0:\n",
    "    initial_epoch = write_cb.start_epoch\n",
    "    print(f'loading old model and restarting from epoch {initial_epoch}')\n",
    "    model = tf.keras.models.load_model(name+'.checkpoint.h5')#, custom_objects=ld.layer_dict)\n",
    "\n",
    "mc_cb = tf.keras.callbacks.ModelCheckpoint(name+'.checkpoint.h5', save_best_only=False)\n",
    "lr_cb = nes.tf.callbacks.ReduceLRWithModelCheckpointAndRecovery(name+'.h5', monitor='loss',\n",
    "                                                                    verbose=2, cooldown=cooldown, \n",
    "                                                                    recovery_decrease_factor=1,\n",
    "                                                                    factor=.8, patience=patience, \n",
    "                                                                    thresh_increase_factor=1.2,\n",
    "                                                                    thresh=thresh,\n",
    "                                                                    save_weights_only=True, \n",
    "                                                                    burn_in=50,\n",
    "                                                                    min_lr=1e-7,\n",
    "                                                                    restart=initial_epoch > 0,\n",
    "                                                                    nloss=5)\n",
    "\n",
    "# callbacks = [lr_cb, write_cb, ne.callbacks.LRLog(), mc_cb]\n",
    "callbacks = [TB_callback,weights_saver]#, ne.callbacks.LRLog(), mc_cb]\n",
    "\n",
    "# callbacks = [lr_cb, write_cb, ne.callbacks.LRLog(), mc_cb]\n",
    "\n",
    "# callbacks = [mc_cb]\n",
    "\n",
    "with tf.device(model_device):\n",
    "    print(\"checking and compiling model:\")\n",
    "    \n",
    "    nes.utils.check_and_compile(model, gen, optimizer=Adam(learning_rate=lr), \n",
    "                                loss=[lfunc], check_layers=False, run_eagerly=True)\n",
    "    \n",
    "    print(f\"{'saving' if dofit else 'loading'} fit results to {lr_cb.fname} and hist to {write_cb.fname}\")\n",
    "    if dofit:\n",
    "        # model.load_weights('aseg.h5')\n",
    "        # fhist = model.fit(gen, epochs=int(num_epochs), steps_per_epoch=50, \n",
    "        #                   initial_epoch=initial_epoch, callbacks=callbacks, \n",
    "        #                   validation_data=vgen, validation_steps=5)\n",
    "        fhist = model.fit(gen, epochs=int(num_epochs), steps_per_epoch=50, \n",
    "                  initial_epoch=initial_epoch, callbacks=callbacks, \n",
    "                  validation_data=vgen, validation_steps=5)\n",
    "    else:\n",
    "        \n",
    "        # model.load_weights(lr_cb.fname)\n",
    "        print(name + '.checkpoint.h5')\n",
    "        model = tf.keras.models.load_model(name + '.checkpoint.h5', custom_objects=ld.layer_dict)\n",
    "    \n",
    "\n",
    "if save_model:\n",
    "    aseg_fname = f'aseg.fscale.{fscale}.h5'\n",
    "    print(f'saving model to {aseg_fname}')\n",
    "    model.save(aseg_fname)\n",
    "\n",
    "\n",
    "vgen = gens.real_gen(mri_man_segs_recoded, mri_norms, vxm_model, norm_atlas, \n",
    "                     None, labels_in, \n",
    "                     batch_size=1, use_rand=True, \n",
    "                     gpuid=synth_gpu, debug=False, add_outside=oshapes)\n",
    "gen = gens.synth_gen(mri_segs_recoded, gen_model, vxm_model, norm_atlas, \n",
    "                     None, labels_in, batch_size=1, use_rand=True, gpuid=synth_gpu, debug=False, \n",
    "                     zero_background=.1,\n",
    "                     add_outside=oshapes)\n",
    "\n",
    "ilist = []\n",
    "olist = []\n",
    "plist = []\n",
    "dlist = []\n",
    "choroid_label = target_lut.search('Left-Choroid')[0]\n",
    "mask = np.ones((nlabels_small,))\n",
    "mask[lesion_label] = 0\n",
    "mask[choroid_label] = 0\n",
    "mask[0] = 0\n",
    "lfunc_dice = ne.losses.Dice(nb_labels=nlabels_small, weights=None, check_input_limits=False).loss\n",
    "# for n in tqdm(range(ntest)):\n",
    "for n in tqdm(range(1)):\n",
    "    inb, outb = next(vgen)\n",
    "    pred = model.predict(inb)\n",
    "    # d = model.evaluate(inb, outb, verbose=0)\n",
    "    d = lfunc_dice(tf.convert_to_tensor(outb, tf.float32), tf.convert_to_tensor(pred, tf.float32))\n",
    "    d = (d.numpy() * mask).sum() / mask.sum()\n",
    "    dlist.append(d)\n",
    "    print(inb[0].shape)\n",
    "    ilist.append(inb[0].squeeze().copy())\n",
    "    olist.append(np.argmax(outb[0].squeeze(), axis=-1).copy())\n",
    "    plist.append(np.argmax(pred[0].squeeze(), axis=-1).copy())\n",
    "\n",
    "\n",
    "print(f'real dice {np.array(dlist).mean()}')\n",
    "print(f'{dlist}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6272c34b-6f60-46bd-b6fb-3b9e650c1683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 192, 192, 192) (1, 192, 192, 192) (1, 192, 192, 192)\n"
     ]
    }
   ],
   "source": [
    "imgs = np.array(ilist)\n",
    "tseg = np.array(olist)\n",
    "pseg = np.array(plist)\n",
    "print(imgs.shape,tseg.shape,pseg.shape)\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# print(imgs[0].shape)\n",
    "for i in range(1):\n",
    "    nib.save(nib.Nifti1Image(imgs[i].astype(np.float32), np.eye(4), header=None), f\"output/image{i}.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(tseg[i].astype(np.int32), np.eye(4), header=None), f\"output/tseg{i}.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(pseg[i].astype(np.int32), np.eye(4), header=None), f\"output/pseg{i}.nii.gz\")\n",
    "\n",
    "# fv = fs.Freeview(swap_batch_dim=True)\n",
    "# fv.vol(imgs, name='img', opts=':locked=1:linked=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf17299-cfa3-4b88-bf52-afa76f05da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 176s 176s/step\n",
      "1/1 [==============================] - 134s 134s/step\n",
      "(1, 192, 192, 192, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [05:33<00:00, 333.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real dice -0.8219432532787323\n",
      "[-0.8219432532787323]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n in tqdm(range(1)):\n",
    "    inb, outb = next(vgen)\n",
    "    pred = model.predict(inb)\n",
    "    # d = model.evaluate(inb, outb, verbose=0)\n",
    "    d = lfunc_dice(tf.convert_to_tensor(outb, tf.float32), tf.convert_to_tensor(pred, tf.float32))\n",
    "    d = (d.numpy() * mask).sum() / mask.sum()\n",
    "    dlist.append(d)\n",
    "    print(inb[0].shape)\n",
    "    ilist.append(inb[0].squeeze().copy())\n",
    "    olist.append(np.argmax(outb[0].squeeze(), axis=-1).copy())\n",
    "    plist.append(np.argmax(pred[0].squeeze(), axis=-1).copy())\n",
    "\n",
    "\n",
    "print(f'real dice {np.array(dlist).mean()}')\n",
    "print(f'{dlist}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0805d11-2697-4373-9cde-b54be7828eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(imgs.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2380e4-50f7-401b-bd5e-9be989f11472",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e102d-6217-4dce-a716-0193515e5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading aseg atlas model from aseg.fscale.1.1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:06<00:00,  6.11it/s]\n",
      "100%|██████████| 41/41 [00:52<00:00,  1.28s/it]\n",
      "  0%|          | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 118s 118s/step\n",
      "1/1 [==============================] - 155s 155s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/41 [05:03<3:22:24, 303.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 115s 115s/step\n",
      "1/1 [==============================] - 164s 164s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/41 [10:12<3:19:26, 306.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 121s 121s/step\n",
      "WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1544e452e3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 164s 164s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/41 [15:28<3:17:01, 311.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 122s 122s/step\n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1544e336bee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 164s 164s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4/41 [20:46<3:13:19, 313.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 117s 117s/step\n",
      "1/1 [==============================] - 162s 162s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5/41 [25:55<3:07:14, 312.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 120s 120s/step\n",
      "1/1 [==============================] - 165s 165s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 6/41 [31:11<3:02:47, 313.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 119s 119s/step\n",
      "1/1 [==============================] - 169s 169s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 7/41 [36:30<2:58:36, 315.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 120s 120s/step\n",
      "1/1 [==============================] - 165s 165s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 8/41 [41:45<2:53:22, 315.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 118s 118s/step\n",
      "1/1 [==============================] - 170s 170s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 9/41 [47:05<2:48:55, 316.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 119s 119s/step\n",
      "1/1 [==============================] - 164s 164s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 10/41 [52:18<2:43:05, 315.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 113s 113s/step\n"
     ]
    }
   ],
   "source": [
    "test_mseg = True\n",
    "if test_mseg:\n",
    "    aseg_atlas_fname = 'aseg.fscale.1.1.h5'\n",
    "    print(f'reading aseg atlas model from {aseg_atlas_fname}')\n",
    "    atlas_model = tf.keras.models.load_model(aseg_atlas_fname, custom_objects=ld.layer_dict)\n",
    "    mapping = fs.lookups.nonlateral_aseg_recoder()\n",
    "    target_lut = mapping.target_lut\n",
    "    lut_name = 'nonlat.txt'\n",
    "    adir = '/autofs/cluster/freesurfer/subjects/atlases/aseg_atlas'\n",
    "    mname = 'seg_edited.mgz'\n",
    "    vname = 'norm.mgz'\n",
    "    sfile = os.path.join(adir, 'scripts', 'subjects.txt')\n",
    "    with open(sfile, 'r') as f:\n",
    "        subjects = f.read().split('\\n')[0:-1]\n",
    "\n",
    "    mri_man_segs = []  # manual segs\n",
    "    mri_norms = []  # mri vols\n",
    "    mri_norms_orig = []\n",
    "    mri_man_segs_orig = []\n",
    "    for s in tqdm(subjects):\n",
    "        # mri_seg_orig = fs.Volume.read(os.path.join(adir, s, 'mri', mname))\n",
    "        mri_seg_orig = sf.load_volume(os.path.join(adir, s, 'mri', mname))\n",
    "        mri_man_segs_orig.append(mri_seg_orig)\n",
    "        # mri_seg = mri_seg_orig.fit_to_shape(target_shape, center='bbox')\n",
    "        mri_seg = mri_seg_orig.reshape(target_shape)\n",
    "        mri_man_segs.append(mri_seg)\n",
    "        mri_norm_orig = sf.load_volume(os.path.join(adir, s, 'mri', vname))\n",
    "        mri_norm = mri_norm_orig.resample_like(mri_seg)\n",
    "        mri_norms.append(mri_norm)\n",
    "        mri_norms_orig.append(mri_norm_orig)\n",
    "\n",
    "    mri_man_segs_recoded = [fs.label.recode(mri, mapping) for mri in tqdm(mri_man_segs)]\n",
    "\n",
    "    mri_seg_atlas = sf.load_volume(\"aseg_atlas.mgz\")\n",
    "    hard_seg = np.argmax(mri_seg_atlas.data, axis=-1)\n",
    "    # mri_hard_seg = mri_seg_atlas.copy(hard_seg).fit_to_shape(target_shape, center='bbox')\n",
    "    mri_hard_seg = sf.Volume(hard_seg).resample_like(mri_seg_atlas).reshape(target_shape)\n",
    "    mri_norm_atlas = sf.load_volume(\"norm_atlas.mgz\").resample_like(mri_hard_seg)\n",
    "    mri_seg_atlas = mri_seg_atlas.resample_like(mri_hard_seg)\n",
    "    norm_atlas = (mri_norm_atlas.data / mri_norm_atlas.data.max())[np.newaxis, ..., np.newaxis]\n",
    "\n",
    "    # psize = (norm_atlas.shape[1] - mri_norms[0].shape[0]) // 2\n",
    "    # pad = ((0,0), (psize,psize), (psize, psize), (psize, psize), (0, 0))\n",
    "    dice_list = []\n",
    "    elist = []\n",
    "    elist_in_atlas = []\n",
    "    alist_in_atlas = []\n",
    "    nlist_in_atlas = []\n",
    "    mlist_in_atlas = []\n",
    "    for sno, s in enumerate(tqdm(subjects)):\n",
    "        mseg_onehot = np.eye(nlabels_small)[mri_man_segs_recoded[sno].data]\n",
    "        norm = (mri_norms[sno].data / mri_norms[sno].data.max())[np.newaxis, ..., np.newaxis]\n",
    "        pred = model.predict(norm)\n",
    "        # transform = gens.vxm_model.predict([l, norm, norm_atlas])\n",
    "        vxm_model = gens.read_vxm_model(inshape)\n",
    "        # smooth_wt_input = KL.Input(vxm_model.inputs[0].shape[1:])\n",
    "        # print(\"vxm_model.inputs[0]\",vxm_model.inputs[0],smooth_wt_input )\n",
    "        l = np.zeros((1, 1))\n",
    "        l[0,0] = .3\n",
    "        #transform = vxm_model.predict([l, norm, norm_atlas])\n",
    "        transform = vxm_model.predict([l, norm, norm_atlas])\n",
    "        # transform = transform[:, psize:-psize, psize:-psize, psize:-psize, :]\n",
    "        dice = lfunc(tf.convert_to_tensor(mseg_onehot[np.newaxis], tf.float32), \n",
    "                     tf.convert_to_tensor(pred, tf.float32))\n",
    "        dice_list.append(dice.numpy())\n",
    "        ev = (mseg_onehot - pred[0])\n",
    "        evol = (ev**2).sum(axis=-1)\n",
    "        elist.append(evol)\n",
    "        evol_in_atlas = vxm.layers.SpatialTransformer(interp_method='linear', fill_value=0)([evol[np.newaxis, ..., np.newaxis], transform])\n",
    "        norm_in_atlas = vxm.layers.SpatialTransformer(interp_method='linear', fill_value=0)([norm, transform])\n",
    "        aseg_in_atlas = vxm.layers.SpatialTransformer(interp_method='linear', fill_value=0)([pred[0][np.newaxis, ...], transform])\n",
    "        mseg_in_atlas = vxm.layers.SpatialTransformer(interp_method='linear', fill_value=0)([mseg_onehot[np.newaxis, ...], transform])\n",
    "        elist_in_atlas.append(evol_in_atlas.numpy().squeeze())\n",
    "        nlist_in_atlas.append(norm_in_atlas.numpy().squeeze())\n",
    "        #alist_in_atlas.append(np.argmax(aseg_in_atlas.numpy(), axis=-1).squeeze())\n",
    "        #mlist_in_atlas.append(np.argmax(mseg_in_atlas.numpy(), axis=-1).squeeze())\n",
    "        alist_in_atlas.append(aseg_in_atlas.numpy().squeeze())\n",
    "        mlist_in_atlas.append(mseg_in_atlas.numpy().squeeze())\n",
    "\n",
    "    evol_avg = np.array(elist_in_atlas).mean(axis=0)\n",
    "    nvol_avg = np.array(nlist_in_atlas).mean(axis=0)\n",
    "    nbhd_img = scipy.ndimage.convolve(evol_avg, np.ones((patch_size,)*3)/(patch_size**3), mode='constant')\n",
    "    mseg_avg = np.argmax(np.array(mlist_in_atlas).mean(axis=0), axis=-1)\n",
    "    aseg_avg = np.argmax(np.array(alist_in_atlas).mean(axis=0), axis=-1)\n",
    "    max_ind = np.argmax(nbhd_img)\n",
    "    print('computing index occurence volumes')\n",
    "    aseg_inds = np.argmax(np.array(alist_in_atlas), axis=-1)\n",
    "    mseg_inds = np.argmax(np.array(mlist_in_atlas), axis=-1)\n",
    "    max_inds = 10\n",
    "    mseg_ind_vol = np.zeros(target_shape + (max_inds,))\n",
    "    aseg_ind_vol = np.zeros(target_shape + (max_inds,))\n",
    "    mseg_num_inds = np.zeros(target_shape)\n",
    "    aseg_num_inds = np.zeros(target_shape)\n",
    "    for x in tqdm(range(aseg_inds.shape[1])):\n",
    "        for y in range(aseg_inds.shape[2]):\n",
    "            for z in range(aseg_inds.shape[3]):\n",
    "                ind_list = -1 * np.ones((max_inds,))\n",
    "                u = np.unique(mseg_inds[:, x, y, z])\n",
    "                for lno, l in enumerate(u):\n",
    "                    if lno >= max_ind:\n",
    "                        break\n",
    "                    mseg_ind_vol[x, y, z, lno] = l\n",
    "                \n",
    "                mseg_num_inds[x, y, z] = len(u)\n",
    "                ind_list = -1 * np.ones((max_inds,))\n",
    "                u = np.unique(aseg_inds[:, x, y, z])\n",
    "                for lno, l in enumerate(u):\n",
    "                    aseg_ind_vol[x, y, z, lno] = l\n",
    "\n",
    "                aseg_num_inds[x, y, z] = len(u)\n",
    "\n",
    "    subs = np.unravel_index(max_ind, nbhd_img.shape)\n",
    "    labels_mseg = np.unique(np.argmax(np.array(mlist_in_atlas), axis=-1)[:, subs[0], subs[1], subs[2]])\n",
    "    labels_aseg = np.unique(np.argmax(np.array(alist_in_atlas), axis=-1)[:, subs[0], subs[1], subs[2]])\n",
    "    print(subs)\n",
    "\n",
    "# fv = fs.Fre\n",
    "    # fv = fs.Freeview()\n",
    "    # fv.vol(mri_segs[0].copy(nvol_avg), name='norm avg')\n",
    "    # fv.vol(mri_segs[0].copy(mseg_avg), name='mseg', opts=':colormap=lut:lut=nonlat')\n",
    "    # fv.vol(mri_segs[0].copy(aseg_avg), name='aseg', opts=':colormap=lut:lut=nonlat')\n",
    "    # nvols = 3\n",
    "    # mvols = np.argmax(np.array(mlist_in_atlas[0:nvols]), axis=-1)\n",
    "    # avols = np.argmax(np.array(alist_in_atlas[0:nvols]), axis=-1)\n",
    "    # fv.vol(mri_segs[0].copy(np.transpose(np.array(nlist_in_atlas[0:nvols]), (1, 2, 3, 0))), name='norm vols')\n",
    "    # fv.vol(mri_segs[0].copy(np.transpose(mvols, (1, 2, 3, 0))), name='man segs', opts=':colormap=lut:lut=nonlat')\n",
    "    # fv.vol(mri_segs[0].copy(np.transpose(avols, (1, 2, 3, 0))), name='synth segs', opts=':colormap=lut:lut=nonlat')\n",
    "    # fv.vol(mri_segs[0].copy(nbhd_img), name='err avg', opts=':colormap=heat:heatscale=.25,.5')\n",
    "    # fv.show(opts=f'-slice {subs[0]} {subs[1]} {subs[2]}', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97300c4e-03ff-4c59-bf54-ec37a4e6802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_hard_seg = sf.Volume(hard_seg).resample_like(mri_seg_atlas).reshape(target_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cacaea-e4d2-456d-9e4a-a1ceb7cd825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imgs[0].shape)\n",
    "nvols = 3\n",
    "mvols = np.argmax(np.array(mlist_in_atlas[0:nvols]), axis=-1)\n",
    "avols = np.argmax(np.array(alist_in_atlas[0:nvols]), axis=-1)\n",
    "volume_data = mri_segs[0].copy(np.transpose(np.array(nlist_in_atlas[0:nvols]), (1, 2, 3, 0))).astype(np.int32)\n",
    "nib.save(nib.Nifti1Image(volume_data, np.eye(4), header=None), \"output/nvol_avg.nii.gz\")\n",
    "\n",
    "# nib.save(nib.Nifti1Image(mri_segs[0].copy(np.transpose(np.array(nlist_in_atlas[0:nvols]), (1, 2, 3, 0))).astype(np.int32), np.eye(4), header=None), \"output/nvol_avg.nii.gz\")\n",
    "\n",
    "# nib.save(nib.Nifti1Image(tseg[i].astype(np.int32), np.eye(4), header=None), f\"output/tseg{i}.nii.gz\")\n",
    "# nib.save(nib.Nifti1Image(pseg[i].astype(np.int32), np.eye(4), header=None), f\"output/pseg{i}.nii.gz\")\n",
    "    # for i in range(imgs.shape[0]):\n",
    "        # nib.save(nib.Nifti1Image(imgs[i].astype(np.int32), np.eye(4), header=None), f\"output/nvol_avg.nii.gz\")\n",
    "        # nib.save(nib.Nifti1Image(tseg[i].astype(np.int32), np.eye(4), header=None), f\"output/tseg{i}.nii.gz\")\n",
    "        # nib.save(nib.Nifti1Image(pseg[i].astype(np.int32), np.eye(4), header=None), f\"output/pseg{i}.nii.gz\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

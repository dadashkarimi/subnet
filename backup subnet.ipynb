{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89244cdd-f4e6-47f0-aee7-bebe40010e7d",
   "metadata": {},
   "source": [
    "# Sub Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6011aaa6-877f-4edb-b899-3afecd000381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host name rtx-08.nmr.mgh.harvard.edu\n",
      "setting dev_str to /cpu:0\n",
      "physical GPU # is None\n",
      "dofit False, doaff True, fit_lin False, oshapes True, subnet loss True, use_lab2ind False, combined_training False, insertion True, concat aseg True, subloss dice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 45590.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model trained with dice loss\n",
      "patch_size = 32, num_subnets = 16, pad_size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 22.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cropping to (192, 192, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]UserWarning: fit_to_shape center argument no longer has any effect\n",
      "100%|██████████| 5/5 [00:00<00:00, 207.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading unique labels in 5 from npy/oasis_labels.npy\n",
      "using warp max = 2 and nlabels 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: nes.utils.generative.random_blur_rescale is deprecated and will be removed in the near future. Please use ne.utils.augment.random_blur_rescale instead.\n",
      "UserWarning: In the future, the default value of argument `clip_max` to nes.models.labels_to_image will change to None.\n",
      "UserWarning: In the future, the default value of argument `slice_stride_min` to nes.models.labels_to_image will change to 1.\n",
      "UserWarning: In the future, the default value of argument `slice_stride_max` to nes.models.labels_to_image will change to 8.\n",
      "UserWarning: In the future, nes.models.labels_to_image will apply the bias field just before adding noise. Use the new code by passing `bias_first=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 192, 192, 192, 1)\n",
      "(None, 192, 192, 192, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:06<00:00,  6.75it/s]\n",
      "100%|██████████| 41/41 [00:48<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 192, 192)\n",
      "(192, 192, 192)\n",
      "norm atlas shape:  (1, 192, 192, 192, 1) (192, 192, 192) (192, 192, 192, 17)\n",
      "loading aseg model aseg_subnet/aseg.fscale.1.1.h5\n",
      "reading cached volumes scale 1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 7/7 [00:14<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading patch centers and labels...\n",
      "loading aseg model aseg_subnet/aseg.fscale.1.1.h5\n",
      "nfeats 60\n",
      "[[60, 60], [60, 60], [60, 60], [60, 60]]\n"
     ]
    }
   ],
   "source": [
    "import socket, os\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers as KL\n",
    "from tqdm import tqdm\n",
    "import glob, copy\n",
    "import scipy\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# from freesurfer import deeplearn as fsd\n",
    "import freesurfer as fs\n",
    "\n",
    "import neurite as ne\n",
    "import neurite_sandbox as nes\n",
    "import voxelmorph as vxm\n",
    "import voxelmorph_sandbox as vxms\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import surfa as sf\n",
    "import generators as gens\n",
    "\n",
    "import layer_dict as ld\n",
    "import pdb as gdb\n",
    "\n",
    "import neurite as ne\n",
    "import neurite_sandbox as nes\n",
    "import voxelmorph as vxm\n",
    "import voxelmorph_sandbox as vxms\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import surfa as sf\n",
    "from utils import *\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "dofit = False\n",
    "ntest = 5\n",
    "num_subjects=5\n",
    "crop = -1 if dofit else ntest\n",
    "# crop = -1\n",
    "\n",
    "log_dir = \"logs_subnets\"\n",
    "models_dir = \"models_subnets_pad_4\"\n",
    "num_epochs = 1000\n",
    "synth_device = '/cpu:0'\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "latest_weight = max(glob.glob(os.path.join(models_dir, 'weights_epoch_*.h5')), key=os.path.getctime, default=None)\n",
    "match = re.search(r'(\\d+)', latest_weight.split(\"/\")[1])\n",
    "initial_epoch = int(match.group())\n",
    "\n",
    "weights_saver = PeriodicWeightsSaver(filepath=models_dir, save_freq=10)  # Save weights every 100 epochs\n",
    "\n",
    "TB_callback = CustomTensorBoard(\n",
    "    base_log_dir=log_dir,\n",
    "    histogram_freq=100,\n",
    "    write_graph=True,\n",
    "    write_images=False,\n",
    "    write_steps_per_second=False,\n",
    "    update_freq='epoch',\n",
    "    profile_batch=0,\n",
    "    embeddings_freq=0,\n",
    "    embeddings_metadata=None\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_path=models_dir+'/weights_epoch_'+str(initial_epoch)+'.h5'\n",
    "\n",
    "mse_wt = 1\n",
    "\n",
    "vscale = 2\n",
    "vscale = 1\n",
    "\n",
    "\n",
    "\n",
    "# whether or not to train the synthseg net from scratch with the subnets\n",
    "combined_training = True\n",
    "combined_training = False\n",
    "\n",
    "# whether or not to concat the (extracted) aseg features into the subnets\n",
    "concat_aseg = False\n",
    "concat_aseg = True\n",
    "\n",
    "# have subnets only output the labels that ever occur in their location or all labels\n",
    "use_lab2ind = True\n",
    "use_lab2ind = False\n",
    "\n",
    "# put losses on the individual subnets or not\n",
    "use_subloss = False\n",
    "use_subloss = True\n",
    "\n",
    "# which optimizer to use\n",
    "which_opt = 'sgd'\n",
    "which_opt = 'adam'\n",
    "\n",
    "# which subnet loss to use (if using a subnet loss)\n",
    "subloss = 'mse'\n",
    "subloss = 'dice'\n",
    "\n",
    "# whether to use the insertion code on the subnet outputs (you always should, wasn't sure at first)\n",
    "use_insertion = False\n",
    "use_insertion = True\n",
    "\n",
    "# whether to freeze the aseg weights when training the subnets\n",
    "train_aseg = False\n",
    "train_aseg = True\n",
    "\n",
    "which_loss = 'both'\n",
    "which_loss = 'mse'\n",
    "which_loss = 'cce'\n",
    "which_loss = 'dice'\n",
    "\n",
    "same_contrast=False\n",
    "same_contrast=True\n",
    "\n",
    "# add synthetic outputs shapes to the images or not\n",
    "oshapes = False\n",
    "oshapes = True\n",
    "\n",
    "# perform linear fitting on inputs to softmax to initialize things\n",
    "fit_lin = True\n",
    "fit_lin = False\n",
    "\n",
    "\n",
    "# do affine augmentation or not\n",
    "doaff = False\n",
    "doaff = True\n",
    "\n",
    "\n",
    "model_dir = 'models'\n",
    "gpuid = -1\n",
    "host = socket.gethostname()\n",
    "from neurite_sandbox.tf.utils.utils import plot_fit_callback as pfc\n",
    "\n",
    "\n",
    "print(f'host name {socket.gethostname()}')\n",
    "\n",
    "# if not dofit and host == 'serena.nmr.mgh.harvard.edu':\n",
    "dev_str = '/cpu:0'\n",
    "print(f'setting dev_str to {dev_str}')\n",
    "\n",
    "\n",
    "print(f'physical GPU # is {os.getenv(\"SLURM_STEP_GPUS\")}')\n",
    "ret = ne.utils.setup_device(dev_str)\n",
    "# policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "\n",
    "print(f'dofit {dofit}, doaff {doaff}, fit_lin {fit_lin}, oshapes {oshapes}, subnet loss {use_subloss}, use_lab2ind {use_lab2ind}, combined_training {combined_training}, insertion {use_insertion}, concat aseg {concat_aseg}, subloss {subloss}')\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "odir = '/autofs/vast/braindev/braindev/OASIS/OASIS1/synth-high-res/recon_subject'\n",
    "subjects = [f for f in Path(odir).iterdir() if 'OASIS_OAS1_0' in str(f)]\n",
    "# seg_files = [f/f'mri/aparc+aseg.mgz' for f in tqdm(subjects)]\n",
    "seg_files = [f/f'mri/aseg.mgz' for f in tqdm(subjects[0:crop])]\n",
    "# seg_files = [f / 'mri/aparc+aseg.mgz' for f in tqdm(subjects[0:crop])]\n",
    "\n",
    "\n",
    "print(f'loading model trained with {which_loss} loss')\n",
    "\n",
    "target_shape = (192,)*3\n",
    "inshape = target_shape\n",
    "\n",
    "\n",
    "# number of subnets and the size of the input patch to each one\n",
    "patch_size = 32\n",
    "num_subnets = 16\n",
    "\n",
    "# initial_epoch = args.initial_epoch\n",
    "\n",
    "pad_size=4\n",
    "# add some padding to the subnets so that there is always some context for each voxel\n",
    "big_patch_size = patch_size + 2 * pad_size\n",
    "phalf = patch_size // 2\n",
    "bphalf = big_patch_size // 2\n",
    "\n",
    "print(f'patch_size = {patch_size}, num_subnets = {num_subnets}, pad_size = {pad_size}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lut = fs.lookups.default()\n",
    "\n",
    "# csf = lut.search('CSF')[0]\n",
    "lesion_label_orig = lut.search('Left-Lesion')\n",
    "if len(lesion_label_orig) > 0:\n",
    "    lesion_label_orig = lesion_label_orig[0]\n",
    "else:   # not in the lut - add a new one\n",
    "    lesion_label_orig = 77\n",
    "    lut.add(lesion_label_orig, 'Lesion', color=[240,240,240])\n",
    "\n",
    "if 'inited' not in locals() and 'inited' not in globals():\n",
    "    inited = False\n",
    "\n",
    "warp_max = 2\n",
    "aseg_train_fscale = 1.1\n",
    "# if not inited:\n",
    "def initialize():\n",
    "    mri_segs_orig = [sf.load_volume(str(fname)) for fname in tqdm(seg_files[:crop])]\n",
    "\n",
    "    if vscale > 1:\n",
    "        print(f'downsampling by {vscale}')\n",
    "        mri_segs = [mri.reslice(vscale) for mri in tqdm(mri_segs_orig)]\n",
    "    else:\n",
    "        print(f'cropping to {target_shape}')\n",
    "        mri_segs = [mri.fit_to_shape(target_shape, center='bbox') for mri in tqdm(mri_segs_orig)]\n",
    "\n",
    "    np_segs_orig = [mri.data for mri in mri_segs]\n",
    "    fname = 'npy/oasis_labels.npy'\n",
    "    if os.path.exists(fname):\n",
    "        print(f'loading unique labels in {len(mri_segs_orig)} from {fname}')\n",
    "        labels_orig = np.load(fname)\n",
    "    else:\n",
    "        print(f'finding unique labels in {len(mri_segs_orig)} datasets...')\n",
    "        labels_orig = np.unique(np.array(np_segs_orig))\n",
    "        np.save(fname, labels_orig)\n",
    "\n",
    "    # mapping = fs.lookups.tissue_type_recoder_no_skull(include_lesions=use_lesions)\n",
    "    mapping = fs.lookups.nonlateral_aseg_recoder()\n",
    "    target_lut = mapping.target_lut\n",
    "    lut_name = 'nonlat.txt'\n",
    "    mri_segs_recoded = [fs.label.recode(mri, mapping) for mri in mri_segs]  \n",
    "    lesion_label = target_lut.search('Left-Lesion')[0]\n",
    "    np_segs = [vol.data for vol in mri_segs_recoded]\n",
    "    labels_in = np.unique(np.array(np_segs)).astype(int)\n",
    "    if lesion_label not in labels_in:\n",
    "        l = list(labels_in)\n",
    "        l.append(lesion_label)\n",
    "        labels_in = np.array(l)\n",
    "    nlabels_small = len(labels_in)\n",
    "    label_map = {}\n",
    "    keys = mapping.mapping.keys()\n",
    "    lab_to_ind = np.zeros((labels_orig.max()+1,), dtype=int)\n",
    "    for label in labels_orig:\n",
    "        if label not in keys:\n",
    "            output_label = 0\n",
    "        else:\n",
    "            output_label = mapping.mapping[label]\n",
    "        label_map[label] = output_label\n",
    "        lab_to_ind[label] = output_label\n",
    "\n",
    "\n",
    "    import generators as gens\n",
    "    vxm_model = gens.read_vxm_model(inshape)\n",
    "    vxm_smooth_wt = np.zeros((1, 1))\n",
    "    vxm_smooth_wt[0,0] = .3   # warp regularization hyper parameter\n",
    "    \n",
    "    gen_model = gens.create_gen_model(np_segs, oshapes, synth_device, nlabels_small, labels_in, inshape, warp_max)\n",
    "\n",
    "    mapping = fs.lookups.nonlateral_aseg_recoder()\n",
    "    target_lut = mapping.target_lut\n",
    "    lut_name = 'nonlat.txt'\n",
    "    adir_2d = '/autofs/cluster/vxmdata1/FS_Slim/proc/cleaned/Buckner39'\n",
    "    adir = '/autofs/cluster/freesurfer/subjects/atlases/aseg_atlas'\n",
    "    mname = 'seg_edited.mgz'\n",
    "    vname = 'norm.mgz'\n",
    "    sfile = os.path.join(adir, 'scripts', 'subjects.txt')\n",
    "    with open(sfile, 'r') as f:\n",
    "        subjects = f.read().split('\\n')[0:-1]\n",
    "\n",
    "\n",
    "    mri_man_segs = []  # manual segs\n",
    "    mri_norms = []  # mri vols\n",
    "    mri_norms_orig = []\n",
    "    mri_man_segs_orig = []\n",
    "\n",
    "\n",
    "    for s in tqdm(subjects):\n",
    "        mri_seg_orig = sf.load_volume(os.path.join(adir, s, 'mri', mname))\n",
    "\n",
    "    \n",
    "        mri_man_segs_orig.append(mri_seg_orig)\n",
    "        mri_seg = mri_seg_orig.reshape(target_shape)\n",
    "        mri_man_segs.append(mri_seg)\n",
    "        mri_norm_orig = sf.load_volume(os.path.join(adir, s, 'mri', vname))\n",
    "\n",
    "        mri_norm = mri_norm_orig.resample_like(mri_seg)\n",
    "        mri_norms.append(mri_norm)\n",
    "        mri_norms_orig.append(mri_norm_orig)\n",
    "        \n",
    "\n",
    "    mri_man_segs_recoded = [fs.label.recode(mri, mapping) for mri in tqdm(mri_man_segs)]\n",
    "\n",
    "    mri_seg_atlas = sf.load_volume(\"aseg_atlas.mgz\").reshape(target_shape)\n",
    "    hard_seg = np.argmax(mri_seg_atlas.data, axis=-1)\n",
    "    mri_hard_seg = mri_seg_atlas.copy()\n",
    "    mri_hard_seg.data = hard_seg\n",
    "    mri_hard_seg_cropped = mri_hard_seg.reshape(target_shape)\n",
    "    print(mri_hard_seg_cropped.shape)\n",
    "    mri_norm_atlas = sf.load_volume(\"norm_atlas.mgz\").resample_like(mri_hard_seg)\n",
    "    print(mri_norm_atlas.shape)\n",
    "\n",
    "    mri_seg_atlas = mri_seg_atlas.resample_like(mri_hard_seg)\n",
    "    norm_atlas = (mri_norm_atlas.data / mri_norm_atlas.data.max())[np.newaxis, ..., np.newaxis]\n",
    "    print(\"norm atlas shape: \", norm_atlas.shape,mri_hard_seg.shape,mri_seg_atlas.shape)\n",
    "    \n",
    "    f = 128\n",
    "    conf = {\n",
    "        'def.enc_nf': [f] * 4,\n",
    "        'def.dec_nf': [f] * 4,\n",
    "        'def.add_nf': [f] * 4,\n",
    "        'def.hyp_den': [32] * 4,\n",
    "    }\n",
    "\n",
    "    vxm_model = vxms.networks.VxmJointAverage(in_shape=inshape, **conf)\n",
    "    vxm_model.load_weights(os.path.join('models_from_Malte', f'VxmJointAverage{f}.h5'))\n",
    "    #aseg_model = tf.keras.models.load_model('aseg.h5', custom_objects=ld.layer_dict)\n",
    "\n",
    "    # if 1:\n",
    "    aseg_train_fname = f'aseg_subnet/aseg.fscale.{aseg_train_fscale}.h5'\n",
    "    print(f'loading aseg model {aseg_train_fname}')\n",
    "    aseg_model = tf.keras.models.load_model(aseg_train_fname, \n",
    "                                            custom_objects=ld.layer_dict)\n",
    "    # else:\n",
    "    #     aseg_model = tf.keras.models.load_model('aseg.h5', custom_objects=ld.layer_dict)\n",
    "\n",
    "    l = np.zeros((1, 1))\n",
    "    l[0,0] = .3   # warp regularization hyper parameter\n",
    "\n",
    "    lfunc = nes.losses.DiceNonzero(nlabels_small, weights=None, check_input_limits=False).loss\n",
    "    read_cached = True\n",
    "    new_cache = True  # fscale specific\n",
    "    # from tqdm import tqdm\n",
    "    # import numpy as np\n",
    "    \n",
    "    # List of files to load\n",
    "    files_to_load = [\n",
    "        'npy/nbhd_img.npy',\n",
    "        f'npy/elist_in_atlas.fscale.{aseg_train_fscale}.npy',\n",
    "        f'npy/nlist_in_atlas.fscale.{aseg_train_fscale}.npy',\n",
    "        f'npy/alist_in_atlas.fscale.{aseg_train_fscale}.npy',\n",
    "        f'npy/mlist_in_atlas.fscale.{aseg_train_fscale}.npy',\n",
    "        f'npy/evol_avg.fscale.{aseg_train_fscale}.npy',\n",
    "        f'npy/nvol_avg.fscale.{aseg_train_fscale}.npy'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    \n",
    "    if new_cache:\n",
    "        print(f'reading cached volumes scale {aseg_train_fscale}')\n",
    "            # Load files with progress bar\n",
    "        loaded_data = []\n",
    "        for file_path in tqdm(files_to_load, desc=\"Loading files\"):\n",
    "            loaded_data.append(np.load(file_path, allow_pickle=True))\n",
    "        # nbhd_img_orig, elist_in_atlas, nlist_in_atlas, alist_in_atlas, mlist_in_atlas, evol_avg, nvol_avg = loaded_data\n",
    "    nbhd_img_orig, elist_in_atlas, nlist_in_atlas, alist_in_atlas, mlist_in_atlas, evol_avg, nvol_avg = loaded_data\n",
    "\n",
    "    return mri_man_segs_recoded, mri_norms, norm_atlas, labels_in, vxm_model, lfunc, aseg_model, gen_model, nbhd_img_orig, elist_in_atlas, nlist_in_atlas, alist_in_atlas, mlist_in_atlas, evol_avg, nvol_avg , nlabels_small , mri_segs_recoded, lesion_label\n",
    "        # nbhd_img_orig = np.load('npy/nbhd_img.npy', allow_pickle=True)\n",
    "        # elist_in_atlas = np.load(f'npy/elist_in_atlas.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "        # nlist_in_atlas = np.load(f'npy/nlist_in_atlas.fscale.{aseg_train_fscale}.npy', allow_pickle=True) \n",
    "        # alist_in_atlas = np.load(f'npy/alist_in_atlas.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "        # mlist_in_atlas = np.load(f'npy/mlist_in_atlas.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "        # evol_avg = np.load(f'npy/evol_avg.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "        # nvol_avg = np.load(f'npy/nvol_avg.fscale.{aseg_train_fscale}.npy', allow_pickle=True)\n",
    "\n",
    "    # inited = True\n",
    "# loaded_data\n",
    "# norm_atlas, labels_in, vxm_model, lfunc, aseg_model , gen_model,  nbhd_img_orig, elist_in_atlas, nlist_in_atlas, alist_in_atlas, mlist_in_atlas, evol_avg, nvol_avg , nlabels_small, mri_segs_recoded, lesion_label = initialize()\n",
    "# nbhd_img_orig, elist_in_atlas, nlist_in_atlas, alist_in_atlas, mlist_in_atlas, evol_avg, nvol_avg = loaded_data\n",
    "mri_man_segs_recoded, \\\n",
    "mri_norms, \\\n",
    "norm_atlas, \\\n",
    "labels_in, \\\n",
    "vxm_model, \\\n",
    "lfunc, \\\n",
    "aseg_model, \\\n",
    "gen_model, \\\n",
    "nbhd_img_orig, \\\n",
    "elist_in_atlas, \\\n",
    "nlist_in_atlas, \\\n",
    "alist_in_atlas, \\\n",
    "mlist_in_atlas, \\\n",
    "evol_avg, \\\n",
    "nvol_avg, \\\n",
    "nlabels_small, \\\n",
    "mri_segs_recoded, \\\n",
    "lesion_label = initialize()\n",
    "\n",
    "nbhd_img = nbhd_img_orig.copy()\n",
    "mseg_avg = np.argmax(np.array(mlist_in_atlas).mean(axis=0), axis=-1)\n",
    "aseg_avg = np.argmax(np.array(alist_in_atlas).mean(axis=0), axis=-1)\n",
    "    \n",
    "mlist_ind = np.argmax(np.array(mlist_in_atlas), axis=-1)\n",
    "alist_ind = np.argmax(np.array(alist_in_atlas), axis=-1)\n",
    "\n",
    "\n",
    "print('loading patch centers and labels...')\n",
    "# patch_labels_files = np.load(f'npy/patch_labels.{patch_size}.{num_subnets}.{pad_size}.npy', allow_pickle=True)\n",
    "# patch_labels = [patch_labels_files[key] for key in patch_labels_files.files]\n",
    "\n",
    "patch_centers = np.load(f'npy/patch_centers.{patch_size}.{num_subnets}.{pad_size}.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "aseg_train_fname = f'aseg_subnet/aseg.fscale.{aseg_train_fscale}.h5'\n",
    "print(f'loading aseg model {aseg_train_fname}')\n",
    "aseg_model = tf.keras.models.load_model(aseg_train_fname, custom_objects=ld.layer_dict)\n",
    "\n",
    "\n",
    "for lno in range(len(aseg_model.layers)-1, 0, -1):\n",
    "    aseg_layer = aseg_model.layers[lno]\n",
    "    if aseg_layer.name.startswith(\"unet_conv_uparm\"):\n",
    "        break\n",
    "\n",
    "nfeats = aseg_layer.output[0].get_shape().as_list()[-1]\n",
    "\n",
    "print(\"nfeats\",nfeats)\n",
    "# gen_model = gens.create_gen_model(np_segs, oshapes, synth_device, nlabels_small, labels_in, inshape)\n",
    "\n",
    "nb_levels = int(np.log2(patch_size))-1\n",
    "nb_levels = int(np.log2(big_patch_size))-1\n",
    "nb_conv_per_level = 2\n",
    "unet_scale = 1\n",
    "    \n",
    "unet_nf = []\n",
    "fscale = 1.1\n",
    "fscale = 1\n",
    "\n",
    "\n",
    "for level in range(nb_levels):\n",
    "    filters_in_this_level = []\n",
    "    for layer in range(nb_conv_per_level):\n",
    "        filters_in_this_level.append(int(fscale**level*nfeats))\n",
    "        \n",
    "    unet_nf.append(filters_in_this_level)\n",
    "\n",
    "print(unet_nf)\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "lr = 1e-5\n",
    "lr = 1e-4\n",
    "name = f'subnets.outside.unet_nf.{nfeats}.warp_max.{warp_max}.oshapes.{oshapes}.num_subnets.{num_subnets}.psize.{patch_size}.pad.{pad_size}.lab2ind.{use_lab2ind}.lr.{lr}.subloss.{use_subloss}.insertion.{use_insertion}.combined_training.{combined_training}.train_aseg.{train_aseg}'\n",
    "if not concat_aseg:\n",
    "    name += f'.concat_aseg.{concat_aseg}'\n",
    "if use_subloss and subloss != 'dice':\n",
    "    name += f'.subloss.{subloss}'\n",
    "if which_opt != 'adam':\n",
    "    name += f'.which_opt.{which_opt}'\n",
    "\n",
    "\n",
    "\n",
    "label_weights = np.ones((1,nlabels_small,))\n",
    "# label_weights[0,-1] = .01  # downweight lesion class\n",
    "lfunc = ne.losses.Dice(nb_labels=nlabels_small, weights=None, check_input_limits=False).mean_loss\n",
    "lfunc = nes.losses.DiceNonzero(nlabels_small, weights=None, check_input_limits=False).loss\n",
    "if use_subloss:\n",
    "    if subloss == 'dice':\n",
    "        thresh = -.2*3\n",
    "    else:\n",
    "        thresh = 8\n",
    "else:\n",
    "        thresh = -.2*2\n",
    "    \n",
    "\n",
    "cooldown = 25\n",
    "patience = 600\n",
    "\n",
    "losses = [lfunc]\n",
    "loss_weights = [1]\n",
    "if use_subloss:\n",
    "    if subloss == 'mse':\n",
    "        lfunc_subnet = lambda a, b: mse_wt * tf.keras.losses.MSE(a, b)\n",
    "    else:\n",
    "        lfunc_subnet = lfunc\n",
    "    #lfunc_mse = tf.keras.losses.mse\n",
    "    losses += [lfunc_subnet]\n",
    "    loss_weights += [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10fc04f1-f004-413e-a549-660864baa866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3073375-1143-490e-8819-d7766a7e1a27",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c10ced-b16a-433e-accd-b6598892ae82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "unet, big_patch_size, noutputs:  [[60, 60], [60, 60], [60, 60], [60, 60]] 40 17\n",
      "checkpoint_path models_subnets_pad_4/weights_epoch_1000.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 155\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(checkpoint_path):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_path\u001b[39m\u001b[38;5;124m\"\u001b[39m,checkpoint_path)\n\u001b[0;32m--> 155\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint file not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:668\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranspose\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;124;03m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m \n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranspose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "synth_gpu = 0\n",
    "model_device = '/cpu:0'\n",
    "synth_device = model_device\n",
    "\n",
    "unet_device = model_device if (fit_lin or dofit) else synth_device\n",
    "with tf.device(unet_device):     # add the subnets to the big unet\n",
    "    aseg_shape = aseg_layer.output.get_shape().as_list()[1:]\n",
    "    aseg_linear_out = aseg_model.layers[-3].output  # tensor right before outputs compressed to nlabels\n",
    "    subnet_outputs_to_add = [aseg_linear_out]  # add subnet outputs to this tensor for final output\n",
    "    subnet_outputs = []  # list of subnet outputs each nlabels_small long (only for using subloss)\n",
    "    subnet_patches = []  # spatial location of the subnet patches\n",
    "    pre_lab2ind = []\n",
    "    \n",
    "    for subnet_no in range(num_subnets):\n",
    "\n",
    "    # for subnet_no in range(num_subnets):\n",
    "        bx0 = patch_centers[subnet_no][0]-big_patch_size//2\n",
    "        bx1 = bx0 + big_patch_size \n",
    "        by0 = patch_centers[subnet_no][1]-big_patch_size//2\n",
    "        by1 = by0 + big_patch_size \n",
    "        bz0 = patch_centers[subnet_no][2]-big_patch_size//2\n",
    "        bz1 = bz0 + big_patch_size \n",
    "        # only inner part without padding\n",
    "        x0 = patch_centers[subnet_no][0]-patch_size//2\n",
    "        x1 = x0 + patch_size \n",
    "        y0 = patch_centers[subnet_no][1]-patch_size//2\n",
    "        y1 = y0 + patch_size \n",
    "        z0 = patch_centers[subnet_no][2]-patch_size//2\n",
    "        z1 = z0 + patch_size \n",
    "        # labels_mseg = np.unique(mlist_ind[:, bx0:bx1, by0:by1, bz0:bz1])\n",
    "        labels_mseg = np.unique(mlist_ind[:, x0:x1, y0:y1, z0:z1])\n",
    "        noutputs = len(labels_mseg) if use_lab2ind else nlabels_small\n",
    "\n",
    "        # extract a patch of (1) the data on the uparm of the big net and (2) the input volume\n",
    "        aseg_patch = nes.layers.ExtractPatch(((bx0, bx1), (by0, by1), (bz0, bz1)), \n",
    "                                             name=f'aseg_patch{subnet_no}')(aseg_layer.output)\n",
    "        patch_input = nes.layers.ExtractPatch(((bx0, bx1), (by0, by1), (bz0, bz1)),\n",
    "                                              name=f'subnet_input{subnet_no}')(aseg_model.inputs[0])\n",
    "        nf = aseg_patch.get_shape().as_list()[-1]\n",
    "        print(\"unet, big_patch_size, noutputs: \",unet_nf,big_patch_size,noutputs)\n",
    "        subnet_lin = ne.models.unet(unet_nf, (big_patch_size,)*3+(1,), None, 3, noutputs, feat_mult=None, final_pred_activation='linear', name=f'subnet{subnet_no}')\n",
    "\n",
    "        if use_lab2ind:\n",
    "            # concat aseg_model info to second-to-last layer of subnet\n",
    "            if concat_aseg:\n",
    "                tmp_model = tf.keras.Model(subnet_lin.inputs, subnet_lin.layers[-3].output)\n",
    "                tmp_out = tmp_model(patch_input)\n",
    "                unet_concat = KL.Concatenate(name=f'subnet_in{subnet_no}', axis=-1)([tmp_out, aseg_patch])\n",
    "                Conv = getattr(KL, 'Conv%dD' % 3)\n",
    "                subnet_out = Conv(noutputs, 3, strides=1, padding='same')(unet_concat)\n",
    "            else:\n",
    "                subnet_out = subnet_lin(patch_input)\n",
    "            \n",
    "            pre_lab2ind.append(subnet_out)\n",
    "            unet_out = nes.layers.IndexToLabel(labels_mseg, nlabels_small, name=f'IndToLab{subnet_no}')(subnet_out)\n",
    "        else:  # what about concatting in the non lab2ind case????\n",
    "            unet_out = subnet_lin(patch_input)\n",
    "\n",
    "        # crop out the beginning and ending pad regions so output is just central patch_size\n",
    "        p0 = pad_size\n",
    "        p1 = pad_size+patch_size\n",
    "        patch_output = nes.layers.ExtractPatch(((p0, p1), (p0, p1), (p0, p1)),\n",
    "                                               name=f'subnet_output{subnet_no}')(unet_out)\n",
    "\n",
    "        if use_subloss:\n",
    "            if subloss == 'dice':\n",
    "                subnet_softmax = KL.Softmax(name=f'subnet{subnet_no}_softmax')(patch_output)\n",
    "                subnet_outputs.append(subnet_softmax[:, tf.newaxis, ...])\n",
    "            else:   # just include the linear output for mse loss\n",
    "                subnet_outputs.append(patch_output[:, tf.newaxis, ...])\n",
    "\n",
    "            subnet_patches.append(((x0, x1), (y0, y1), (z0, z1)))\n",
    "\n",
    "        if subnet_no == 0 or not use_insertion:\n",
    "            from keras.layers import ZeroPadding3D\n",
    "\n",
    "            padding = ((x0, aseg_shape[0] - x1), (y0, aseg_shape[1] - y1), (z0, aseg_shape[2] - z1))\n",
    "            padded_unet_output = ZeroPadding3D(padding=padding, name=f'pad_subnet{subnet_no}')(patch_output)\n",
    "\n",
    "            if not use_insertion:\n",
    "                subnet_outputs_to_add.append(padded_unet_output)\n",
    "            else:   # initialize one big tensor with patch outputs\n",
    "                big_patches_output = padded_unet_output\n",
    "        else:  # just insert this patch (since they don't overlap don't need to add)\n",
    "            offset = ((x0, y0, z0, 0))\n",
    "            big_patches_output = nes.layers.InsertPatch(big_patches_output, offset, \n",
    "                                                         name=f'patch_insert{subnet_no}')([\n",
    "                                                             patch_output, big_patches_output])\n",
    "            \n",
    "\n",
    "    if not use_insertion:\n",
    "        summed_aseg_outputs = KL.Add(name='patch_plus_unet')(subnet_outputs_to_add)\n",
    "    else:\n",
    "        summed_patch_outputs = KL.Lambda(lambda x: x, name='summed_patch_outputs')(big_patches_output)\n",
    "        summed_aseg_outputs = KL.Add(name='patch_plus_unet')([summed_patch_outputs, aseg_linear_out])\n",
    "\n",
    "    model_lin = tf.keras.Model(aseg_model.inputs, [summed_aseg_outputs])\n",
    "    softmax_out = KL.Softmax(name='seg')(summed_aseg_outputs)\n",
    "    outputs = [softmax_out]\n",
    "    if use_subloss:\n",
    "        subnet_out = KL.Concatenate(name='subloss', axis=1)(subnet_outputs)\n",
    "        outputs += [subnet_out]\n",
    "    model = tf.keras.Model(aseg_model.inputs, outputs)\n",
    "\n",
    "\n",
    "# create the training (synth) and validation (on real data) generators\n",
    "with tf.device(synth_device):\n",
    "    val_size = 20\n",
    "    gen = gens.synth_gen(mri_segs_recoded, gen_model, vxm_model, norm_atlas, \n",
    "                         None, labels_in, \n",
    "                         batch_size=batch_size, \n",
    "                         subnet_patches=subnet_patches if use_subloss else None,\n",
    "                         use_log_for_subnet=subloss == 'mse',\n",
    "                         use_rand=True, gpuid=synth_gpu, debug=False, add_outside=oshapes)\n",
    "    vgen = gens.real_gen(mri_man_segs_recoded, mri_norms, vxm_model, norm_atlas, \n",
    "                         None, labels_in, \n",
    "                         batch_size=batch_size, use_rand=True,\n",
    "                         use_log_for_subnet=subloss == 'mse',\n",
    "                         subnet_patches=subnet_patches if use_subloss else None,\n",
    "                         gpuid=synth_gpu, debug=False, add_outside=oshapes)\n",
    "\n",
    "\n",
    "\n",
    "# set this to nonzero if restarting training from an interrupted run\n",
    "initial_epoch = 1\n",
    "initial_epoch = 0\n",
    "\n",
    "if not use_subloss:\n",
    "    key_replacements = {\n",
    "        'loss' : 'seg_loss',\n",
    "        'val_loss' : 'val_seg_loss'\n",
    "    }\n",
    "else:\n",
    "    key_replacements = None\n",
    "\n",
    "\n",
    "# opt = keras.optimizers.Adam(learning_rate=lr) if which_opt == 'adam' else keras.optimizers.SGD(learning_rate=lr)\n",
    "# nes.utils.check_and_compile(model, gen, optimizer=opt, \n",
    "#                             loss=losses, loss_weights=loss_weights, check_layers=False, run_eagerly=True)\n",
    "\n",
    "# callbacks = [TB_callback,weights_saver]\n",
    "\n",
    "# checkpoint = tf.train.Checkpoint(model=model,optim=model.optimizer)\n",
    "# checkpoint_manager = tf.train.CheckpointManager(checkpoint, f'{name}.saved_model', max_to_keep = 5)\n",
    "# cp = checkpoint_manager.restore_or_initialize()\n",
    "# if initial_epoch > 0:\n",
    "    # status = checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "    \n",
    "with tf.device(model_device):\n",
    "    if not train_aseg:\n",
    "        aseg_model.trainable = False\n",
    "        \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(\"checkpoint_path\",checkpoint_path)\n",
    "        model.load_weights(checkpoint_path)\n",
    "    else:\n",
    "\n",
    "        print(\"Checkpoint file not found.\")\n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f684b1-e635-4c29-b788-2f89f8c565fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ilist = []\n",
    "olist = []\n",
    "plist = []\n",
    "dlist = []\n",
    "choroid_label = target_lut.search('Left-Choroid')[0]\n",
    "mask = np.ones((nlabels_small,))\n",
    "mask[lesion_label] = 0\n",
    "mask[choroid_label] = 0\n",
    "mask[0] = 0\n",
    "lfunc_dice = ne.losses.Dice(nb_labels=nlabels_small, weights=None, check_input_limits=False).loss\n",
    "# for n in tqdm(range(ntest)):\n",
    "for n in tqdm(range(1)):\n",
    "    inb, outb = next(vgen)\n",
    "    pred = model.predict(inb)\n",
    "    # d = model.evaluate(inb, outb, verbose=0)\n",
    "    d = lfunc_dice(tf.convert_to_tensor(outb, tf.float32), tf.convert_to_tensor(pred, tf.float32))\n",
    "    d = (d.numpy() * mask).sum() / mask.sum()\n",
    "    dlist.append(d)\n",
    "    print(inb[0].shape)\n",
    "    ilist.append(inb[0].squeeze().copy())\n",
    "    olist.append(np.argmax(outb[0].squeeze(), axis=-1).copy())\n",
    "    plist.append(np.argmax(pred[0].squeeze(), axis=-1).copy())\n",
    "\n",
    "\n",
    "print(f'real dice {np.array(dlist).mean()}')\n",
    "print(f'{dlist}')\n",
    "\n",
    "imgs = np.array(ilist)\n",
    "tseg = np.array(olist)\n",
    "pseg = np.array(plist)\n",
    "print(imgs.shape,tseg.shape,pseg.shape)\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "# print(imgs[0].shape)\n",
    "for i in range(1):\n",
    "    nib.save(nib.Nifti1Image(imgs[i].astype(np.float32), np.eye(4), header=None), f\"output/image{i}.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(tseg[i].astype(np.int32), np.eye(4), header=None), f\"output/tseg{i}.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(pseg[i].astype(np.int32), np.eye(4), header=None), f\"output/pseg{i}.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2f9898-05b9-4469-9c5f-cbf16a4abaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, copy\n",
    "import socket, os\n",
    "models_dir = \"models_subnets\"\n",
    "\n",
    "latest_weight = max(glob.glob(os.path.join(models_dir, 'weights_epoch_*.h5')), key=os.path.getctime, default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0e90eef-0264-4f51-b5ef-b5ad527fa964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models_subnets/weights_epoch_0.h5'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b1180-d62c-4629-8a48-ac90c29542cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
